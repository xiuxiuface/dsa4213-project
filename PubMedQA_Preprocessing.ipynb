{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up"
      ],
      "metadata": {
        "id": "rStVfJQ-gGew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "import ast\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Ensure punkt is downloaded\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "S-mtiJJ1fnMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6926f05e-14db-4aa2-8d2b-685319b93f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter"
      ],
      "metadata": {
        "id": "f-g_0pYZAHl5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLb3Z185LL4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55b4e1e4-584b-4be5-ffe4-5edf1f3d4c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load PubMedQA\n",
        "pubmed = pd.read_parquet(\"hf://datasets/qiaojin/PubMedQA/pqa_artificial/train-00000-of-00001.parquet\")\n",
        "pubmed_raw_df = pubmed[['question', 'context', 'long_answer', \"final_decision\"]]\n",
        "pubmed_raw_df.columns = ['question', 'context', 'answer', \"final_decision\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract context from context dictionary\n",
        "\n",
        "# Function to extract 'contexts' from each row\n",
        "def extract_contexts(row):\n",
        "\n",
        "    row_dict = row['context']\n",
        "\n",
        "    # Get the list of contexts\n",
        "    return \" \".join(row_dict.get(\"contexts\", []))  # join all contexts as a single string\n",
        "\n",
        "# Function to extract 'contexts' from each row\n",
        "def extract_meshes(row):\n",
        "\n",
        "    row_dict = row['context']\n",
        "\n",
        "    # Get the list of meshes — return as list (not string)\n",
        "    return row_dict.get(\"meshes\", [])\n",
        "\n",
        "# Apply to the dataframe\n",
        "pubmed_raw_df['extracted_contexts'] = pubmed_raw_df.progress_apply(extract_contexts, axis=1)\n",
        "pubmed_raw_df['meshes'] = pubmed_raw_df.progress_apply(extract_meshes, axis=1)\n",
        "pubmed_raw_df = pubmed_raw_df.drop('context', axis=1)"
      ],
      "metadata": {
        "id": "xLRVG6uFi6K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter to diabetes related QAs\n",
        "diabetes_mesh_terms = [\n",
        "    \"diabetes mellitus\",\n",
        "    \"diabetes mellitus, type 1\",\n",
        "    \"diabetes mellitus, type 2\",\n",
        "    \"gestational diabetes\",\n",
        "    \"prediabetic state\",\n",
        "    \"hyperglycemia\",\n",
        "    \"hypoglycemia\",\n",
        "    \"insulin resistance\",\n",
        "    \"glucose intolerance\",\n",
        "    \"glucose metabolism disorders\",\n",
        "    \"glycated hemoglobin a\",\n",
        "    \"hba1c\",\n",
        "    \"insulin\",\n",
        "    \"metformin\",\n",
        "    \"diabetic retinopathy\",\n",
        "    \"diabetic nephropathy\",\n",
        "    \"diabetic neuropathy\",\n",
        "    \"diabetic foot\",\n",
        "    \"diabetic ketoacidosis\",\n",
        "    \"metabolic syndrome\"\n",
        "]\n",
        "\n",
        "def is_diabetes_related(meshes):\n",
        "    \"\"\"Check if MeSH terms contain diabetes-related keywords.\"\"\"\n",
        "    if isinstance(meshes, list):\n",
        "        mesh_text = \" \".join(meshes).lower()\n",
        "    else:\n",
        "        mesh_text = str(meshes).lower()\n",
        "    return any(term in mesh_text for term in diabetes_mesh_terms)\n",
        "\n",
        "# Apply filter\n",
        "tqdm.pandas(desc=\"Filtering for diabetes-related MeSH terms\")\n",
        "pubmed_raw_df[\"is_diabetes_related\"] = pubmed_raw_df[\"meshes\"].progress_apply(is_diabetes_related)\n",
        "\n",
        "# Create diabetes-only subset\n",
        "diabetes_df = pubmed_raw_df[pubmed_raw_df[\"is_diabetes_related\"]].reset_index(drop=True).drop(['is_diabetes_related', 'meshes'], axis=1)\n",
        "\n",
        "diabetes_df.to_excel(\"raw_diabetes_df.xlsx\", index=False)\n",
        "\n",
        "print(f\"Extracted {len(diabetes_df)} diabetes-related samples out of {len(pubmed_raw_df)} total.\")"
      ],
      "metadata": {
        "id": "Y_9bWhdli6ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning"
      ],
      "metadata": {
        "id": "OZoI9Bcg2G4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data cleaning\n",
        "\n",
        "def remove_irrelevant(text):\n",
        "    # Iteratively remove parentheses that contain URLs, NCT ids, DOI, ClinicalTrials, etc.\n",
        "    paren_pattern = re.compile(\n",
        "        r'\\s*\\([^()]*?(?:https?://|www\\.|\\.gov\\b|\\.edu\\b|\\.org\\b|\\.com\\b|\\.net\\b|doi|clinicaltrials|nct\\d+)[^()]*\\)',\n",
        "        flags=re.IGNORECASE\n",
        "    )\n",
        "    # remove inner-most matches repeatedly so nested parentheses are handled\n",
        "    while True:\n",
        "        new_text = paren_pattern.sub('', text)\n",
        "        if new_text == text:\n",
        "            break\n",
        "        text = new_text\n",
        "\n",
        "    # Normalise all caps text\n",
        "    acronyms = [\"PCOS\", \"ICSI\", \"HbA1c\", \"HIV\", \"BMI\", \"DNA\", \"RNA\", \"mRNA\", \"IRCT\"]\n",
        "    # If mostly uppercase, convert to sentence case\n",
        "    if re.search(r'[A-Z]{3,}', text) and text.isupper():\n",
        "        text = text.lower().capitalize()\n",
        "    # Restore known acronyms\n",
        "    for acronym in acronyms:\n",
        "        text = re.sub(r'\\b' + acronym.lower() + r'\\b', acronym, text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove anything in parentheses containing \"ABSTRACT TRUNCATED\"\n",
        "    text = re.sub(r'\\s*\\([^)]*(ABSTRACT TRUNCATED|REFERENCE NUMBER|REGISTRATION NUMBER)[^)]*\\)', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove empty parentheses that may remain (e.g. \"()\")\n",
        "    text = re.sub(r'\\(\\s*\\)', '', text)\n",
        "\n",
        "    # Remove entire sentences that contain standalone websites (not in parentheses)\n",
        "    #    (after removing parentheses above, these are truly standalone)\n",
        "    text = re.sub(\n",
        "        r'[^.?!]*\\b(?:https?://\\S+|www\\.\\S+|\\S+\\.gov|\\S+\\.edu|\\S+\\.org|\\S+\\.com|\\S+\\.net)\\b[^.?!]*[.?!]',\n",
        "        '',\n",
        "        text,\n",
        "        flags=re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    # Remove any reference lines\n",
        "    # Escape special regex chars in phrase, to match literally\n",
        "    pattern = re.compile(r'[^.?!]*\\b' + re.escape(\"GAUK No\") + r'\\b[^.?!]*[.?!]', re.IGNORECASE)\n",
        "    text = re.sub(pattern, '', text)\n",
        "\n",
        "    # Remove phrases like \"Chinese Abstract.\" or similar\n",
        "    text = re.sub(r':?\\s*[A-Z][a-z]+\\sAbstract\\.', '', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def clean_symbols(text):\n",
        "    # Unicode normalization\n",
        "    text = re.sub(r'\\s+x\\s+', ' × ', text)\n",
        "\n",
        "    # Replace textual operator variants with Unicode forms\n",
        "    # Handle \"<=\" and \"< or =\" variants → ≤\n",
        "    text = re.sub(r'<\\s*(?:or\\s*=|/?=|=)', '≤', text, flags=re.IGNORECASE)\n",
        "    # Handle \">=\" and \"> or =\" variants → ≥\n",
        "    text = re.sub(r'>\\s*(?:or\\s*=|/?=|=)', '≥', text, flags=re.IGNORECASE)\n",
        "    # Handle \"+=\" and \"+ or =\" variants → ±\n",
        "    text = re.sub(r'\\+\\s*(?:or\\s*-|/?-|-)', '±', text, flags=re.IGNORECASE)\n",
        "    text = text.replace('=>', '≥').replace('=<', '≤')\n",
        "\n",
        "    # Fix incomplete decimals\n",
        "    text = re.sub(r'(?<![\\d\\w])\\.(\\d+)', r'0.\\1', text)\n",
        "    text = re.sub(r'(?<=\\d)·(?=\\d)', '.', text)  # only replace middle dots between digits\n",
        "\n",
        "    # Keep % tight to preceding number\n",
        "    text = re.sub(r'(\\d)\\s*%', r'\\1%', text)\n",
        "\n",
        "    # Normalize parentheses/brackets\n",
        "    text = re.sub(r'\\(\\s+', '(', text)\n",
        "    text = re.sub(r'\\s+\\)', ')', text)\n",
        "\n",
        "    # Clean up leftover spacing / stray punctuation\n",
        "    # remove space before punctuation, collapse multiple spaces/newlines\n",
        "    text = re.sub(r'\\s+([.,;:?!])', r'\\1', text)\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def normalize_bullets(text):\n",
        "    # Remove bullet \"•\" points\n",
        "    text = re.sub(r'•\\s*', '', text)\n",
        "\n",
        "    # Replace \"i. e. \" with \"i.e. \"\n",
        "    text = re.sub(r'\\b\\(?i\\.\\s*e\\.\\s?\\b', \"i.e. \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\b\\(?e\\.\\s*g\\.\\s?\\b', 'e.g. ', text, flags=re.IGNORECASE)\n",
        "\n",
        "    pattern = re.compile(\n",
        "        r'(?:(?<=^)|(?<=\\n)|(?<=\\. ))'  # start of line, after newline, or after \". \"\n",
        "        r'(?<![=])'                     # not immediately preceded by =\n",
        "        r'(?<![A-Za-z0-9])'             # not preceded by a letter or digit\n",
        "        r'(?<![.)-])'                   # not preceded by ., ), or -\n",
        "        r'(?<!\\= )'                      # not preceded by \"= \" (equals + space)\n",
        "        r'\\(?'\n",
        "        r'([0-9]+|[a-z]|[ivx]+)'        # digits, letters, or roman numerals\n",
        "        r'\\)?'\n",
        "        r'[.)]'\n",
        "        r'(?=\\s)'                        # must be followed by a space\n",
        "    )\n",
        "\n",
        "    text = re.sub(pattern, \"\", text)\n",
        "\n",
        "    # Clean multiple spaces\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def clean_question(text):\n",
        "    text = text.strip()\n",
        "\n",
        "    # Remove spaces before ?\n",
        "    text = re.sub(r'\\s+\\?', '?', text)\n",
        "\n",
        "    # Add ? if missing\n",
        "    if text and not text.endswith(\"?\"):\n",
        "        text += \"?\"\n",
        "\n",
        "    # Capitalize first letter\n",
        "    if text:\n",
        "        text = text[0].upper() + text[1:]\n",
        "\n",
        "    # Remove everything between ':' and '?'\n",
        "    text = re.sub(r'\\s*:\\s*[^?]*\\?', '?', text)\n",
        "\n",
        "    # Fix spacing before punctuation\n",
        "    text = re.sub(r'\\s+:', r':', text)\n",
        "\n",
        "    # Normalize comma spacing: ensure \", \" format\n",
        "    text = re.sub(r'\\s*,\\s*', ', ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def ensure_final_punctuation(text):\n",
        "    text = text.rstrip()  # remove trailing whitespace\n",
        "    # Remove spaces before final period if present\n",
        "    text = re.sub(r'\\s+\\.$', '.', text)\n",
        "    if text and text[-1] not in '.!?':\n",
        "        text += '.'\n",
        "    return text\n",
        "\n",
        "def general_cleaning(text):\n",
        "    \"\"\"\n",
        "    General text cleaning:\n",
        "    1. Remove numbers at the end of a sentence.\n",
        "    2. Collapse multiple spaces into one (excluding line breaks).\n",
        "    \"\"\"\n",
        "    # Remove numbers at the end of lines\n",
        "    text = re.sub(r'\\b\\d+\\b(?=\\s*$)', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Collapse multiple spaces into one (excluding line breaks)\n",
        "    text = re.sub(r'[ ]{2,}', ' ', text)\n",
        "\n",
        "    # Strip leading/trailing spaces on each line\n",
        "    text = \"\\n\".join(line.strip() for line in text.split(\"\\n\"))\n",
        "\n",
        "    # Remove any \":\" or \";\" at the start or end\n",
        "    text = re.sub(r'^[;:]+|[;:]+$', '', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def preprocess_qa(df):\n",
        "    processed_questions = []\n",
        "    processed_answers = []\n",
        "    processed_contexts = []\n",
        "\n",
        "    # Initialize tqdm progress bar\n",
        "    tqdm.pandas(desc=\"Processing Q&A pairs\")\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Cleaning QAs\", ncols=100):\n",
        "        question = row['question']\n",
        "        answer = row['answer']\n",
        "        context = row['extracted_contexts']\n",
        "\n",
        "        # Clean question\n",
        "        question = clean_question(question)\n",
        "\n",
        "        # Puntuating\n",
        "        answer = ensure_final_punctuation(answer)\n",
        "        context = ensure_final_punctuation(context)\n",
        "\n",
        "        # Remove irrelevant metadata\n",
        "        answer = remove_irrelevant(answer)\n",
        "        context = remove_irrelevant(context)\n",
        "\n",
        "        # Normalize all symbols\n",
        "        answer = clean_symbols(answer)\n",
        "        context = clean_symbols(context)\n",
        "\n",
        "        # Standardize bullets (including line-break bullets)\n",
        "        answer = normalize_bullets(answer)\n",
        "        context = normalize_bullets(context)\n",
        "\n",
        "        # General cleaning step\n",
        "        question = general_cleaning(question)\n",
        "        answer = general_cleaning(answer)\n",
        "        context = general_cleaning(context)\n",
        "\n",
        "        # Puntuating\n",
        "        answer = ensure_final_punctuation(answer)\n",
        "        context = ensure_final_punctuation(context)\n",
        "\n",
        "        processed_questions.append(question)\n",
        "        processed_answers.append(answer)\n",
        "        processed_contexts.append(context)\n",
        "\n",
        "    df['question_clean'] = processed_questions\n",
        "    df['answer_clean'] = processed_answers\n",
        "    df['context_clean'] = processed_contexts\n",
        "\n",
        "    # Drop rows where cleaned question or answer is blank\n",
        "    df = df[(df['question_clean'].str.strip() != '') & (df['answer_clean'].str.strip() != '') & (df['context_clean'].str.strip() != '')]\n",
        "\n",
        "    # Drop duplicates based on cleaned question and answer\n",
        "    df = df.drop_duplicates(subset=['question_clean', 'answer_clean', 'context_clean']).reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "pubmed_clean_df = preprocess_qa(diabetes_df)\n",
        "pubmed_clean_df.to_excel(\"pubmed_clean.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "mPSF36CBzXVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QA and summarisation"
      ],
      "metadata": {
        "id": "pUL-4XnMbyPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_all_datasets(df_clean, sentences_per_passage=3):\n",
        "    # Initialize lists\n",
        "    qa_rows = []\n",
        "    summarization_rows = []\n",
        "    rag_rows = []\n",
        "\n",
        "    passage_counter = 0\n",
        "    question_counter = 0\n",
        "\n",
        "    for _, row in tqdm(df_clean.iterrows(), total=len(df_clean), desc=\"Processing data\", ncols=100):\n",
        "        question = row['question_clean']\n",
        "        context = row['context_clean']\n",
        "        answer = row['answer_clean']\n",
        "        final_decision = row['final_decision']\n",
        "\n",
        "        if pd.isna(question) or pd.isna(context) or pd.isna(answer):\n",
        "            continue\n",
        "\n",
        "        # QA dataset\n",
        "        qa_rows.append({'question': question, 'context': context, 'answer': answer, 'final_decision': final_decision})\n",
        "\n",
        "        # Summarisation dataset\n",
        "        summarization_rows.append({'context': context, 'summary': answer})\n",
        "\n",
        "        # RAG dataset: split context into passages\n",
        "        sentences = sent_tokenize(context)\n",
        "        for i in range(0, len(sentences), sentences_per_passage):\n",
        "            passage_sentences = sentences[i:i+sentences_per_passage]\n",
        "            passage_text = \" \".join(passage_sentences).strip()\n",
        "\n",
        "            if passage_text:  # skip empty passages\n",
        "                rag_rows.append({\n",
        "                    'question_id': question_counter,\n",
        "                    'question': question,\n",
        "                    'final_decision': final_decision,\n",
        "                    'answer': answer,\n",
        "                    'passage': passage_text,\n",
        "                    'passage_id': passage_counter\n",
        "                })\n",
        "                passage_counter += 1\n",
        "\n",
        "        question_counter += 1\n",
        "\n",
        "    # Create DataFrames\n",
        "    qa_dataset = pd.DataFrame(qa_rows).drop_duplicates().reset_index(drop=True)\n",
        "    summarization_dataset = pd.DataFrame(summarization_rows).drop_duplicates().reset_index(drop=True)\n",
        "    rag_dataset = pd.DataFrame(rag_rows).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    return qa_dataset, summarization_dataset, rag_dataset\n",
        "\n",
        "qa_data, summarization_data, rag_data = create_all_datasets(pubmed_clean_df, sentences_per_passage=3)\n",
        "\n",
        "# Save to Excel\n",
        "qa_data.to_excel(\"qa_dataset.xlsx\", index=False)\n",
        "summarization_data.to_excel(\"summarization_dataset.xlsx\", index=False)\n",
        "rag_data.to_excel(\"rag_dataset.xlsx\", index=False)\n"
      ],
      "metadata": {
        "id": "bkiv5D2YufVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}