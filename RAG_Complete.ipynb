{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Summarization - Complete Pipeline\n",
    "\n",
    "## üìã Project Workflow\n",
    "1. ‚úÖ Set up retrieval (BM25 & FAISS dense)\n",
    "2. ‚úÖ Implement RAG pipeline\n",
    "3. ‚úÖ Ablation studies (retriever type, top-k)\n",
    "4. ‚úÖ Evaluate QA & summarisation, compute readability metrics\n",
    "5. ‚úÖ Log and compare ablation results\n",
    "\n",
    "## üìä Evaluation Metrics\n",
    "- ROUGE-1, ROUGE-2, ROUGE-L\n",
    "- F1 BERTScore\n",
    "- Avg Flesch-Kincaid Grade\n",
    "- Individual FK grades (in CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Installation (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q rank-bm25 sentence-transformers faiss-cpu transformers torch rouge-score bert-score textstat pandas numpy tqdm openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Retrieval\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Generation\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Evaluation\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import textstat\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_excel(\"rag_dataset.xlsx\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus (unique passages)\n",
    "unique_passages = df.drop_duplicates(subset=[\"passage_id\"])\n",
    "corpus = unique_passages[\"passage\"].tolist()\n",
    "passage_ids = unique_passages[\"passage_id\"].tolist()\n",
    "\n",
    "print(f\"Total unique passages: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Retriever Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 Retriever\n",
    "class BM25Retriever:\n",
    "    def __init__(self, corpus: List[str]):\n",
    "        print(\"Initializing BM25...\")\n",
    "        tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        self.corpus = corpus\n",
    "        print(\"‚úì BM25 ready\")\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[str]:\n",
    "        tokenized_query = query.lower().split()\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        top_k_indices = np.argsort(scores)[::-1][:k]\n",
    "        return [self.corpus[i] for i in top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS Dense Retriever\n",
    "class FAISSRetriever:\n",
    "    def __init__(self, corpus: List[str], model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        print(f\"Initializing FAISS with {model_name}...\")\n",
    "        self.embed_model = SentenceTransformer(model_name)\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        # Create embeddings\n",
    "        print(\"Creating passage embeddings...\")\n",
    "        self.passage_embeddings = self.embed_model.encode(\n",
    "            corpus, \n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        # Build FAISS index\n",
    "        dimension = self.passage_embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # Inner Product\n",
    "        \n",
    "        # Normalize for cosine similarity\n",
    "        faiss.normalize_L2(self.passage_embeddings)\n",
    "        self.index.add(self.passage_embeddings)\n",
    "        print(f\"‚úì FAISS index ready with {self.index.ntotal} vectors\")\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[str]:\n",
    "        query_vec = self.embed_model.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(query_vec)\n",
    "        scores, indices = self.index.search(query_vec, k)\n",
    "        return [self.corpus[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Initialize Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BM25\n",
    "bm25_retriever = BM25Retriever(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FAISS (this may take a few minutes)\n",
    "faiss_retriever = FAISSRetriever(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Summarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationModel:\n",
    "    def __init__(self, model_name: str = \"google/flan-t5-base\"):\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
    "        print(f\"‚úì Model loaded on {self.device}\")\n",
    "    \n",
    "    def generate(self, question: str, context: List[str], max_length: int = 150) -> str:\n",
    "        context_str = \" \".join(context)\n",
    "        prompt = f\"Question: {question}\\n\\nContext: {context_str}\\n\\nSummarize the answer based on the context:\"\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Initialize\n",
    "summarizer = SummarizationModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Quick Test (Â∞èÊ†∑Êú¨ÊµãËØï)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with one example\n",
    "test_row = df.drop_duplicates(subset=[\"question_id\"]).iloc[0]\n",
    "\n",
    "print(\"Question:\", test_row[\"question\"])\n",
    "print(\"\\nTrue answer:\", test_row[\"answer\"])\n",
    "\n",
    "# BM25\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BM25 Retrieval (k=3):\")\n",
    "bm25_ctx = bm25_retriever.retrieve(test_row[\"question\"], k=3)\n",
    "bm25_summary = summarizer.generate(test_row[\"question\"], bm25_ctx)\n",
    "print(\"Summary:\", bm25_summary)\n",
    "print(\"FK Grade:\", textstat.flesch_kincaid_grade(bm25_summary))\n",
    "\n",
    "# FAISS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FAISS Retrieval (k=3):\")\n",
    "faiss_ctx = faiss_retriever.retrieve(test_row[\"question\"], k=3)\n",
    "faiss_summary = summarizer.generate(test_row[\"question\"], faiss_ctx)\n",
    "print(\"Summary:\", faiss_summary)\n",
    "print(\"FK Grade:\", textstat.flesch_kincaid_grade(faiss_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    def __init__(self):\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL'], \n",
    "            use_stemmer=True\n",
    "        )\n",
    "    \n",
    "    def evaluate_batch(self, predictions: List[str], references: List[str]) -> Tuple[Dict, List]:\n",
    "        # ROUGE scores\n",
    "        rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            scores = self.rouge_scorer.score(ref, pred)\n",
    "            rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "            rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "            rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "        \n",
    "        # BERTScore\n",
    "        P, R, F1 = bert_score(predictions, references, lang=\"en\", verbose=False)\n",
    "        \n",
    "        # Flesch-Kincaid\n",
    "        fk_grades = [textstat.flesch_kincaid_grade(p) for p in predictions]\n",
    "        \n",
    "        metrics = {\n",
    "            \"ROUGE-1\": np.mean(rouge_scores['rouge1']),\n",
    "            \"ROUGE-2\": np.mean(rouge_scores['rouge2']),\n",
    "            \"ROUGE-L\": np.mean(rouge_scores['rougeL']),\n",
    "            \"BERTScore_F1\": F1.mean().item(),\n",
    "            \"Avg_FK_Grade\": np.mean(fk_grades)\n",
    "        }\n",
    "        \n",
    "        return metrics, fk_grades\n",
    "\n",
    "evaluator = RAGEvaluator()\n",
    "print(\"‚úì Evaluator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Ablation Study (Ê∂àËûçÁ†îÁ©∂)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample test set for ablation\n",
    "TEST_SIZE = 500  # Adjust based on your computational resources\n",
    "test_df = df.drop_duplicates(subset=[\"question_id\"]).sample(n=TEST_SIZE, random_state=42)\n",
    "\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation configurations\n",
    "configs = [\n",
    "    {\"name\": \"BM25_k3\", \"retriever\": bm25_retriever, \"k\": 3},\n",
    "    {\"name\": \"BM25_k5\", \"retriever\": bm25_retriever, \"k\": 5},\n",
    "    {\"name\": \"BM25_k10\", \"retriever\": bm25_retriever, \"k\": 10},\n",
    "    {\"name\": \"FAISS_k3\", \"retriever\": faiss_retriever, \"k\": 3},\n",
    "    {\"name\": \"FAISS_k5\", \"retriever\": faiss_retriever, \"k\": 5},\n",
    "    {\"name\": \"FAISS_k10\", \"retriever\": faiss_retriever, \"k\": 10},\n",
    "]\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {config['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=config['name']):\n",
    "        ctx = config['retriever'].retrieve(row['question'], k=config['k'])\n",
    "        pred = summarizer.generate(row['question'], ctx)\n",
    "        predictions.append(pred)\n",
    "        references.append(row['answer'])\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics, _ = evaluator.evaluate_batch(predictions, references)\n",
    "    \n",
    "    result = {\n",
    "        \"config_name\": config['name'],\n",
    "        \"retriever_type\": config['name'].split('_')[0],\n",
    "        \"k\": config['k'],\n",
    "        **metrics\n",
    "    }\n",
    "    ablation_results.append(result)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Save results\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "ablation_df = pd.DataFrame(ablation_results)\n",
    "ablation_df.to_csv(\"results/ablation_study.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ABLATION STUDY COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "ablation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Full Evaluation (ÂÆåÊï¥ËØÑ‰º∞)\n",
    "\n",
    "‰ΩøÁî® ablation study ‰∏≠Ë°®Áé∞ÊúÄÂ•ΩÁöÑÈÖçÁΩÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best config from ablation\n",
    "best_config = ablation_df.loc[ablation_df['BERTScore_F1'].idxmax()]\n",
    "print(\"Best configuration:\")\n",
    "print(best_config)\n",
    "\n",
    "# Use the best retriever and k\n",
    "BEST_RETRIEVER = faiss_retriever if \"FAISS\" in best_config['config_name'] else bm25_retriever\n",
    "BEST_K = int(best_config['k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full evaluation on entire dataset\n",
    "print(\"Running full evaluation...\")\n",
    "print(f\"Using: {best_config['config_name']}\")\n",
    "\n",
    "unique_df = df.drop_duplicates(subset=[\"question_id\"]).reset_index(drop=True)\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for _, row in tqdm(unique_df.iterrows(), total=len(unique_df), desc=\"Full evaluation\"):\n",
    "    ctx = BEST_RETRIEVER.retrieve(row['question'], k=BEST_K)\n",
    "    pred = summarizer.generate(row['question'], ctx)\n",
    "    predictions.append(pred)\n",
    "    references.append(row['answer'])\n",
    "\n",
    "# Add predictions to dataframe\n",
    "unique_df['rag_summary'] = predictions\n",
    "\n",
    "# Evaluate\n",
    "final_metrics, fk_grades = evaluator.evaluate_batch(predictions, references)\n",
    "unique_df['FK_grade'] = fk_grades\n",
    "\n",
    "# Save\n",
    "unique_df.to_csv(\"results/rag_full_outputs.csv\", index=False)\n",
    "\n",
    "with open(\"results/rag_full_metrics.json\", \"w\") as f:\n",
    "    json.dump(final_metrics, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL METRICS\")\n",
    "print(\"=\"*80)\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úì Saved to results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Extract Required Passages (ÊèêÂèñÁâπÂÆöÊÆµËêΩ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific passages\n",
    "required_passages = [16771, 12220, 29568]\n",
    "\n",
    "subset_df = unique_df[unique_df['passage_id'].isin(required_passages)].copy()\n",
    "subset_df.to_csv(\"results/rag_required_passages.csv\", index=False)\n",
    "\n",
    "print(\"Required passages extracted:\")\n",
    "subset_df[['passage_id', 'question', 'rag_summary', 'FK_grade']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "### Generated Files:\n",
    "1. `results/ablation_study.csv` - Comparison of different retrievers and k values\n",
    "2. `results/rag_full_outputs.csv` - All predictions with individual FK grades\n",
    "3. `results/rag_full_metrics.json` - Overall metrics (ROUGE, BERTScore, Avg FK)\n",
    "4. `results/rag_required_passages.csv` - Specific passages [16771, 12220, 29568]\n",
    "\n",
    "### Metrics in JSON:\n",
    "- ROUGE-1\n",
    "- ROUGE-2\n",
    "- ROUGE-L\n",
    "- BERTScore_F1\n",
    "- Avg_FK_Grade ‚Üê **Average across all samples**\n",
    "\n",
    "### Individual FK Grades:\n",
    "- Stored in CSV files (column: `FK_grade`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
