{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtYqp4O70OiI"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TaCxyrQ527fV"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import textstat\n",
        "from textstat import flesch_kincaid_grade\n",
        "from scipy.stats import bootstrap\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from rouge_score import rouge_scorer\n",
        "import bert_score\n",
        "from bert_score import score, plot_example\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OHWtlYVlvaj1"
      },
      "outputs": [],
      "source": [
        "# Read files\n",
        "qa_val = pd.read_excel(\"qa_validation.xlsx\")\n",
        "qa_test = pd.read_excel(\"qa_test.xlsx\")\n",
        "\n",
        "summ_val = pd.read_excel(\"summarization_validation.xlsx\")\n",
        "summ_test = pd.read_excel(\"summarization_test.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1RJsjAn1EWG"
      },
      "source": [
        "# Zero shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "snrb1haB1DN8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345,
          "referenced_widgets": [
            "79991aec78684d568beceedf0918f936",
            "7b6a1afc98504ed58e78245b6a2c3751",
            "7669e45a271645cebca80adbb7036d66",
            "a510649f98064840b912463a1d76cf58",
            "54ba4c7e2a494e14a104477c3a0af9b7",
            "65760fa174f54d3ca184121f4bf4abc5",
            "a32921298d99490aae2601f4bd99a2e3",
            "c0b3125fc4964f1cb0bf626fabf54252",
            "c595dbcf0cd54695bd10055bbddc278a",
            "7b30d51482674e8a9bef86f027eb3f2e",
            "a243713c76654a7ba90ef27c573c57fc",
            "bfb52769a9ec4fd99b4ff18655060a95",
            "373340d9cebb48f0a2f59381fe75f742",
            "e92d014590174b49909cb033c3e9512b",
            "f5492875426c41fe83b89075a7856194",
            "a3775955484548ed81086676c4c56cbd",
            "f83707bcf1df433b8169256ae96ad685",
            "9afa4096c8ee433eaa9b559a08f954ea",
            "c40cc3ff3cd94436bb99883768af809c",
            "06c4acd7cc0b4b94bdb9eeafbff294c2",
            "8ef084cf1c1b4a868734749870900b16",
            "dea1f42b1f754ba691bc600b603122c0",
            "dfd3959b7657431ead8cf4ee3ee8eae6",
            "f6c04fc98ff74f7294d869a2b538141c",
            "d63913982c854bf38d43e9cda01bdbc4",
            "799f80340b0b4378b9262ac5ada9d617",
            "f2a68817b3d64fc2939d8ba7cf395ce5",
            "f7a06b4696ff4b56a3ef60c4133c9fbe",
            "876977550d994256ab81231cec895a5b",
            "0aebe343aea94fa2895fd0c43d56c5c4",
            "ab460b4d387042ebb7dac676a60f5f78",
            "3f01bd0e2bbb4ffd8bcdd656529283a6",
            "5f762813159246dd88d408470775d7f5",
            "7c1088c430254959bd042ca6074404a2",
            "5412eee9d9034dd08b7d1ca57f77a276",
            "3ea8d17035bc4981b5bd31eb9c69ff1c",
            "81a5eb0c349f4bd0b2764f0f688c9a18",
            "42ad4a5099cf4d9d83e7d9bfc38c8f2d",
            "e99e3e68cb2f4f5488caec9ed615c6e7",
            "f05e13baa1c0484d8d99f38e76865cb2",
            "961470be5f6f4f27b0062f6d50a0a578",
            "276d42e058a6474da7da05eceac41d4f",
            "c84d6ab07f884314932aa64cb943a5ee",
            "dfb952b62b054074a28ce3b094bcfd21",
            "93ed673a7aef4e57ac7cb9c334672fd3",
            "927eee65b216478597ecc2f71a1f5c1d",
            "22c69f024ac54634ab4e08bc17cd4fad",
            "b5476b058f5245588897a5a13dee555f",
            "e23b9dcbdeed4600aadd0fd6a3e9ca7a",
            "05db767e5f9748b3bd3fafb666538947",
            "c67f624434c248cd9d16b1ff47618879",
            "f253549da8244a0cbb20692d6e9ab58c",
            "a106784ed0e64e96804532067dabb3bd",
            "e17c734977b842c28d156c77bf681bc5",
            "60f8729e1d814ae99ad0cbee796a395f",
            "3dd2dd0f45b746a0bc310cd8384f350f",
            "bfe8b9b09ee54d91a55fb108ee469ab9",
            "801e90b401a54980a3aef1c8769ea6df",
            "88cbb09ad5d84ff994f1482335da46af",
            "ac3bf17eb8774e748f1be84c99c56fdf",
            "125b5184e6dc461a8117e11f6949bd10",
            "8793781f749b46eeb2a52696bec4cc90",
            "88ae63f1c236474dbc7abddc3f6f2aa6",
            "4ded09f71604454b85dfd8723492b338",
            "116ac16f6cda4ce0937b14c1d0c591ba",
            "03aa4f38592d4a7284c9305edbf4d718",
            "42f69688ec654afbbf586f6e0a8ef508",
            "32c3757609bc490dad4ef3ba1847d6e3",
            "c9ec2ee045104d2d98466e3a23f1999b",
            "4ffea7b73ef844dba3122947ebb9777f",
            "e7df5f21998047a0b54f17c980fb5a49",
            "746fcf8e0eb940699d6a9551af50258e",
            "20d6e96f8d9e44f991f9a29d5f19a4c5",
            "0564d1ac2a1348fc9729ea4b74a7f0f4",
            "fb3c78d3e063497f9c84db4c6505aa5d",
            "5b789e524be042ab8fd6cac43b2a6ea1",
            "4e53f72b465b46d8bf3ed6235255124a"
          ]
        },
        "outputId": "a9f5353d-d19c-48b5-b794-eb38fa15c89a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79991aec78684d568beceedf0918f936"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfb52769a9ec4fd99b4ff18655060a95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfd3959b7657431ead8cf4ee3ee8eae6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c1088c430254959bd042ca6074404a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93ed673a7aef4e57ac7cb9c334672fd3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dd2dd0f45b746a0bc310cd8384f350f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42f69688ec654afbbf586f6e0a8ef508"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load model\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "\n",
        "\n",
        "# Build Prompt Function\n",
        "def build_prompt(row, task=\"qa\", prompt_type=\"plain\"):\n",
        "    qa_prompts = {\n",
        "        \"plain\": (\n",
        "            \"Answer the biomedical question accurately. Respond only with 'yes' or 'no'.\\n\\n\"\n",
        "            f\"Question: {row['question']}\\nAnswer:\"\n",
        "        ),\n",
        "        \"cite_source\": (\n",
        "            \"Answer the biomedical question accurately while citing possible biomedical sources. Respond only with 'yes' or 'no'.\\n\\n\"\n",
        "            f\"Question: {row['question']}\\nAnswer:\"\n",
        "        ),\n",
        "        \"context\": (\n",
        "            \"Answer the biomedical question using the context given below. Respond only with 'yes' or 'no'.\\n\\n\"\n",
        "            f\"Context: {row['context']}\\nQuestion: {row['question']}\\nAnswer:\"\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    summarisation_prompts = {\n",
        "        \"plain\": (\n",
        "            \"Summarise the following biomedical text concisely:\\n\\n\"\n",
        "            f\"{row['context']}\"\n",
        "        ),\n",
        "        \"cite_source\": (\n",
        "            \"Summarise the following biomedical text concisely while citing possible biomedical sources:\\n\\n\"\n",
        "            f\"{row['context']}\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "    if task == \"qa\":\n",
        "        return qa_prompts.get(prompt_type, qa_prompts[\"plain\"])\n",
        "    elif task == \"summarisation\":\n",
        "        return summarisation_prompts.get(prompt_type, summarisation_prompts[\"plain\"])\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown task type: {task}\")\n",
        "\n",
        "\n",
        "# Generate Outputs Function\n",
        "def generate_outputs(df, task=\"qa\", prompt_type=\"plain\",\n",
        "                     max_input_len=512, max_output_len=128, num_beams=4, temperature=0.0):\n",
        "    results = []\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        prompt = build_prompt(row, task, prompt_type)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_len).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if task == \"qa\":\n",
        "                output_tokens = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=5, # short yes/no\n",
        "                    num_beams=num_beams,\n",
        "                    do_sample=True, # allow stochasticity\n",
        "                    temperature=0.7, # some randomness to avoid collapsing\n",
        "                )\n",
        "            elif task == \"summarisation\":\n",
        "                output_tokens = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_output_len,\n",
        "                    num_beams=num_beams,\n",
        "                    do_sample=False,\n",
        "                    temperature=temperature\n",
        "                )\n",
        "\n",
        "        generated = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "        results.append(generated.strip())\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def add_preds_to_df(df, all_preds):\n",
        "    df_copy = df.copy()\n",
        "    for prompt_type, preds in all_preds.items():\n",
        "        df_copy[f'pred_{prompt_type}'] = preds\n",
        "    return df_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWB8cxtqqQ24",
        "outputId": "802bdf79-c89b-4db9-b744-afd68cc10a43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2061/2061 [01:39<00:00, 20.69it/s]\n",
            "100%|██████████| 2061/2061 [01:43<00:00, 19.91it/s]\n",
            "100%|██████████| 2061/2061 [01:39<00:00, 20.65it/s]\n",
            "  0%|          | 0/2061 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "100%|██████████| 2061/2061 [22:50<00:00,  1.50it/s]\n",
            "100%|██████████| 2061/2061 [23:13<00:00,  1.48it/s]\n"
          ]
        }
      ],
      "source": [
        "# QA\n",
        "qa_results = {\n",
        "    prompt_type: generate_outputs(qa_test, task=\"qa\", prompt_type=prompt_type)\n",
        "    for prompt_type in [\"plain\", \"cite_source\", \"context\"]\n",
        "}\n",
        "\n",
        "qa_test_with_preds = add_preds_to_df(qa_test, qa_results)\n",
        "\n",
        "# Summarisation\n",
        "summ_results = {\n",
        "    prompt_type: generate_outputs(qa_test, task=\"summarisation\", prompt_type=prompt_type)\n",
        "    for prompt_type in [\"plain\", \"cite_source\"]#, \"simple\", \"length\"]\n",
        "}\n",
        "\n",
        "summ_test_with_preds = add_preds_to_df(summ_test, summ_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FwkPIs228OA7"
      },
      "outputs": [],
      "source": [
        "qa_test_with_preds.to_excel(\"flant5_qa_test_pred.xlsx\", index=False)\n",
        "summ_test_with_preds.to_excel(\"flant5_summ_test_pred.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDl8lhRt6VCr"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cjnL0fwo6TtA"
      },
      "outputs": [],
      "source": [
        "# Decision-Based QA Metrics (Yes/No)\n",
        "def compute_qa_decision_metrics(preds, golds):\n",
        "    results = []\n",
        "    for p, g in zip(preds, golds):\n",
        "        p_clean, g_clean = str(p).lower().strip(), str(g).lower().strip()\n",
        "        em = int(p_clean == g_clean)\n",
        "        f1 = f1_score([g_clean], [p_clean], average='macro', labels=['yes','no'], zero_division=0)\n",
        "        results.append({\"Exact Match\": em, \"F1_macro\": f1})\n",
        "    return results\n",
        "\n",
        "\n",
        "# Summarisation Metrics\n",
        "def compute_text_generation_metrics(preds, golds):\n",
        "    # Include all 3 ROUGE variants\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    smoothie = SmoothingFunction().method1\n",
        "\n",
        "    # Pre-compute BERTScore\n",
        "    P, R, F1 = bert_score.score(preds, golds, lang='en', rescale_with_baseline=True)\n",
        "    bert_f1s = F1.tolist()\n",
        "\n",
        "    results = []\n",
        "    for p, g, bf1 in zip(preds, golds, bert_f1s):\n",
        "        p, g = str(p).strip(), str(g).strip()\n",
        "        if not p or not g:\n",
        "            results.append({\n",
        "                \"ROUGE-1\": 0, \"ROUGE-2\": 0, \"ROUGE-L\": 0,\n",
        "                \"BLEU\": 0, \"BERTScore_F1\": 0\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        rouge_scores = scorer.score(g, p)\n",
        "        rouge1 = rouge_scores['rouge1'].fmeasure\n",
        "        rouge2 = rouge_scores['rouge2'].fmeasure\n",
        "        rougeL = rouge_scores['rougeL'].fmeasure\n",
        "        bleu = sentence_bleu([g.split()], p.split(), smoothing_function=smoothie)\n",
        "\n",
        "        results.append({\n",
        "            \"ROUGE-1\": rouge1,\n",
        "            \"ROUGE-2\": rouge2,\n",
        "            \"ROUGE-L\": rougeL,\n",
        "            \"BLEU\": bleu,\n",
        "            \"BERTScore_F1\": bf1\n",
        "        })\n",
        "    return results\n",
        "\n",
        "\n",
        "# Readability (Flesch-Kincaid)\n",
        "def compute_readability(preds):\n",
        "    results = []\n",
        "    for p in preds:\n",
        "        p = str(p).strip()\n",
        "        grade = textstat.flesch_kincaid_grade(p) if p else None\n",
        "        results.append({\"Flesch-Kincaid Grade\": grade})\n",
        "    return results\n",
        "\n",
        "\n",
        "# Combined Evaluation Wrapper\n",
        "def evaluate_all_prompts(task, df, all_preds):\n",
        "    df_combined = df.copy()\n",
        "    all_averages = {}\n",
        "\n",
        "    if task == \"qa\":\n",
        "        for pt, preds in all_preds.items():\n",
        "            col_pred = f\"pred_{pt}\"\n",
        "            df_combined[col_pred] = preds\n",
        "\n",
        "            golds = df['final_decision'].tolist()\n",
        "            row_metrics = compute_qa_decision_metrics(preds, golds)\n",
        "\n",
        "            # Convert metrics to DataFrame and add suffix\n",
        "            metrics_df = pd.DataFrame(row_metrics).add_suffix(f\"_{pt}\")\n",
        "            df_combined = pd.concat([df_combined, metrics_df], axis=1)\n",
        "\n",
        "            # Compute dataset-level averages\n",
        "            avg_metrics = {k: np.nanmean([m[k] for m in row_metrics if m[k] is not None])\n",
        "                           for k in row_metrics[0].keys()}\n",
        "            all_averages[pt] = avg_metrics\n",
        "\n",
        "    elif task == \"summarisation\":\n",
        "        for pt, preds in all_preds.items():\n",
        "            col_pred = f\"pred_{pt}\"\n",
        "            df_combined[col_pred] = preds\n",
        "\n",
        "            golds = df['summary'].tolist()\n",
        "            gen_metrics = compute_text_generation_metrics(preds, golds)\n",
        "            read_metrics = compute_readability(preds)\n",
        "\n",
        "            row_metrics = [{**g, **r} for g, r in zip(gen_metrics, read_metrics)]\n",
        "\n",
        "            # Convert metrics to DataFrame and add suffix\n",
        "            metrics_df = pd.DataFrame(row_metrics).add_suffix(f\"_{pt}\")\n",
        "            df_combined = pd.concat([df_combined, metrics_df], axis=1)\n",
        "\n",
        "            # Compute dataset-level averages\n",
        "            avg_metrics = {k: np.nanmean([m[k] for m in row_metrics if m[k] is not None])\n",
        "                           for k in row_metrics[0].keys()}\n",
        "            all_averages[pt] = avg_metrics\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid task. Choose 'qa' or 'summarisation'\")\n",
        "\n",
        "    return df_combined, all_averages\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For QA\n",
        "qa_metrics, qa_average = evaluate_all_prompts(task=\"qa\", df=qa_test, all_preds=qa_results)\n",
        "\n",
        "# For summarisation\n",
        "summ_metrics, summ_average = evaluate_all_prompts(task=\"summarisation\", df=summ_test, all_preds=summ_results)\n",
        "\n",
        "print(\"QA:\")\n",
        "print(qa_average)\n",
        "\n",
        "print(\"\\nSummarisation:\")\n",
        "print(summ_average)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385,
          "referenced_widgets": [
            "2ce1e553289e4a818144fcb6496a0bf9",
            "552cf4aaa5444705998089832a72ca10",
            "27edc9dcf51046c79824d3b317a67f50",
            "3e69ba8b7cda44eb9d3395a78b3a0fdd",
            "5d751a14091c4dd2ae9aa476e43e78ca",
            "50532db206d54cdbb71d3fdfec622521",
            "4809669f6094480099127a070b6dec9a",
            "8856a74c7caa474a9318fd0827dd8581",
            "5f3c72ab58574c768aa54ab69c937320",
            "1dee878aee4d4205b735ab44a4179ab7",
            "6bc5384a45544906946cbbaa56cc5173",
            "718e65c463c24dc68c91ef91af8d5f00",
            "0ea3909f5eb04bfaab167a5089337b53",
            "7f500e612ac34ea4b95147a9ae0dc440",
            "2ca007d5abf341ceaedc2e631be068c5",
            "4d823e524562432daee2f007d4c75774",
            "3b0c031e76eb4f35a3b819b6465ccc83",
            "ebc5a2bbceb6458a950dddfbbddfb7b1",
            "6f4d7d3588fb4ec3a7cfaf678a10c330",
            "9e96ec077cf5466cb6b4435b9cd392a2",
            "bf1f952865764279a12339cba2e8b6d5",
            "c38521bfc9094daead4df6284029d052",
            "4903f85b7f5541bdb95ab71fb75b6131",
            "0c22a686e7724271a1fba066086e7bc1",
            "136deabf495440d08d484ebd22250961",
            "5569ac6affbe4c689533da515696180d",
            "052856fa5d4d42d2afb447187b6cae09",
            "e0fe842550044d98a0291d0aea588e1b",
            "719bba507aa048909815d7bcef046e1c",
            "fd65aed798744b17b4efcd390955332c",
            "2a7143a98ff44c309aab0fb8ba80119e",
            "9ecc6ad6f39c4f1bb85a7205ff1a252d",
            "5f3ba2fa23104715b198cf480a526e3e",
            "83e9243265f545b4850e1412ab68dd61",
            "179d85761c354bdaabd72bc9c633bc0c",
            "bfa2ccbf32294202829ffba6bb6e602c",
            "8efb7b9fc41f4090aceef8689c4b92d8",
            "7857019dfc0b4cf9842cae6b6b964a42",
            "5580ac4ecbc24801a11ddeb4966b20b5",
            "6e7332f452964ec2a0500633668bf8f7",
            "ab4245fe783f414c89f86a1a18278378",
            "cfb7ee90d3a0436a97d25f82db0bba7b",
            "a794289030124a8bbd47491dd716652a",
            "03ccf8dcefdd441db007b2bd8a42096e",
            "c8994db268894dcd9490c071e0d182e3",
            "476f65c7616548b3a6a6d7a5e5bd28d4",
            "5f221a0123b84d728d50cf0ccad8d861",
            "7f8351df8f9e47faa05ec52be47c8790",
            "9646718b3e9a4d9db576d30d05d62896",
            "5e6614f866af4f9f8f1db50329c8ea4a",
            "949d5cd3a72c48b2a4307163c3f2c3ce",
            "1610eba4161b4aa89ea5bce6d28064a7",
            "954387a1260848e3b6e496d05c4e6937",
            "94c279ed39394096ba18b8b5ead93ee3",
            "2d7227c167bc4be6996aea73d56611af",
            "3ee1d4e0a0b747dda8faabbfa29c7eaa",
            "e181fc4f27fa43fe9f9cb4cf9eb7d436",
            "b1eb6c47673244f1b4f64de6c20bb3ca",
            "71582a8d300148b8b73ba5f308e52290",
            "34fdffdb2499473a95b00b5256e1a21e",
            "1288234152374d98a131a7e9fc55caa6",
            "8f60b306fa2e4ba4af487ea22f80fc9a",
            "becc849d0e5943c5b5cf2a37f1d57bbf",
            "76643f96a2cd4143989bc226ed7749cc",
            "cfe89d02cf8f454f8d09cff8dda24360",
            "25bb5b5fb2fb422496e6b6e319e2ba8b"
          ]
        },
        "id": "M8zOi5ErmSLP",
        "outputId": "0b9d4136-bed6-4dfd-efac-c55533aaad9d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ce1e553289e4a818144fcb6496a0bf9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "718e65c463c24dc68c91ef91af8d5f00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4903f85b7f5541bdb95ab71fb75b6131"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83e9243265f545b4850e1412ab68dd61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8994db268894dcd9490c071e0d182e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ee1d4e0a0b747dda8faabbfa29c7eaa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QA:\n",
            "{'plain': {'Exact Match': np.float64(0.07520621057738962), 'F1_macro': np.float64(0.03760310528869481)}, 'cite_source': {'Exact Match': np.float64(0.07569141193595343), 'F1_macro': np.float64(0.03784570596797671)}, 'context': {'Exact Match': np.float64(0.39349830179524503), 'F1_macro': np.float64(0.19674915089762252)}}\n",
            "\n",
            "Summarisation:\n",
            "{'plain': {'ROUGE-1': np.float64(0.2975839167715025), 'ROUGE-2': np.float64(0.1166292202830598), 'ROUGE-L': np.float64(0.23003547169016306), 'BLEU': np.float64(0.03657864091773953), 'BERTScore_F1': np.float64(0.2172157031173338), 'Flesch-Kincaid Grade': np.float64(16.217845640039418)}, 'cite_source': {'ROUGE-1': np.float64(0.29481374637945496), 'ROUGE-2': np.float64(0.1164209798538296), 'ROUGE-L': np.float64(0.22746190773909233), 'BLEU': np.float64(0.036291886827414535), 'BERTScore_F1': np.float64(0.21071225221815945), 'Flesch-Kincaid Grade': np.float64(16.045159834324906)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_metrics.to_excel(\"flant5_qa_test_metrics.xlsx\", index=False)\n",
        "summ_metrics.to_excel(\"flant5_summ_test_metrics.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "4FSnfP_U9XPE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few Shot"
      ],
      "metadata": {
        "id": "_9GSWsK4sZdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_few_shot_qa_prompt(row, qa_val, n_examples=2):\n",
        "    examples = qa_val.sample(n=n_examples, random_state=int(row.name))  # random 3 examples\n",
        "    prompt = \"Answer the biomedical questions with either 'yes' or 'no':\\n\\n\"\n",
        "\n",
        "    for i, ex in enumerate(examples.itertuples(), start=1):\n",
        "        prompt += f\"Context: {ex.context}\\nQuestion: {ex.question}\\nAnswer: {ex.final_decision}\\n\\n\"\n",
        "\n",
        "    # Add target question\n",
        "    #prompt += f\"Answer with only 'yes' or 'no' to the following biomedical question:\\n\"\n",
        "    prompt += f\"Context: {row['context']}\\nQuestion: {row['question']}\\nAnswer:\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def build_few_shot_summ_prompt(row, summ_val, n_examples=2):\n",
        "    examples = summ_val.sample(n=n_examples, random_state=int(row.name))  # random 3 examples\n",
        "    prompt = \"Summarise the biomedical texts:\\n\\n\"\n",
        "\n",
        "    for i, ex in enumerate(examples.itertuples(), start=1):\n",
        "        prompt += f\"Text: {ex.context}\\nSummary: {ex.summary}\\n\\n\"\n",
        "\n",
        "    # Add target question\n",
        "    #prompt += f\"Now, summarise the following biomedical text concisely:\\n\"\n",
        "    prompt += f\"Text: {row['context']}\\nSummary:\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def generate_outputs_fewshot(df, task=\"qa\", max_input_len=1024, max_output_len=128,\n",
        "                     num_beams=4, temperature=0.0):\n",
        "    results = []\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        # Build the prompt\n",
        "        if task == \"qa\":\n",
        "            prompt = build_few_shot_qa_prompt(row, qa_val)\n",
        "        elif task == \"summarisation\":\n",
        "            prompt = build_few_shot_summ_prompt(row, summ_val)\n",
        "        else:\n",
        "            raise ValueError(\"task must be 'qa' or 'summarisation'\")\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_len).to(model.device)\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            output_tokens = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_output_len,\n",
        "                num_beams=num_beams if task==\"summarisation\" else 1, #greedy for qa\n",
        "                do_sample=(task==\"summarisation\"),          # sampling for summarisation only\n",
        "                temperature=0.3 if task==\"summarisation\" else temperature\n",
        "            )\n",
        "\n",
        "        # Decode and store result\n",
        "        generated = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "        results.append(generated.strip())\n",
        "    return results"
      ],
      "metadata": {
        "id": "ygfINwRu0Hc2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate few-shot outputs for QA\n",
        "qa_few_shot_results = generate_outputs_fewshot(qa_test, task=\"qa\")\n",
        "\n",
        "# Generate few-shot outputs for Summarisation\n",
        "summ_few_shot_results = generate_outputs_fewshot(summ_test, task=\"summarisation\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeF2PsKDHPxR",
        "outputId": "ee4e867a-dcf2-4fae-f0b7-011860739f6f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2061/2061 [02:07<00:00, 16.20it/s]\n",
            "100%|██████████| 2061/2061 [24:44<00:00,  1.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add few-shot predictions to the test dataframes\n",
        "qa_test_with_few_shot_preds = add_preds_to_df(qa_test, {\"few_shot\": qa_few_shot_results})\n",
        "\n",
        "summ_test_with_few_shot_preds = add_preds_to_df(summ_test, {\"few_shot\": summ_few_shot_results})"
      ],
      "metadata": {
        "id": "V9goJtmAJRCJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For QA\n",
        "qa_few_shot_metrics, qa_few_shot_average = evaluate_all_prompts(task=\"qa\", df=qa_test, all_preds={\"few_shot\": qa_few_shot_results})\n",
        "\n",
        "# For summarisation\n",
        "summ_few_shot_metrics, summ_few_shot_average = evaluate_all_prompts(task=\"summarisation\", df=summ_test, all_preds={\"few_shot\": summ_few_shot_results})\n",
        "\n",
        "print(\"QA:\")\n",
        "print(qa_few_shot_average)\n",
        "\n",
        "print(\"\\nSummarisation:\")\n",
        "print(summ_few_shot_average)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql1K1YIHMw6i",
        "outputId": "84661b3b-e053-4cb7-a05d-582f71502d4f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QA:\n",
            "{'few_shot': {'Exact Match': np.float64(0.17515769044153323), 'F1_macro': np.float64(0.08757884522076662)}}\n",
            "\n",
            "Summarisation:\n",
            "{'few_shot': {'ROUGE-1': np.float64(0.2587252191674944), 'ROUGE-2': np.float64(0.09261584416063948), 'ROUGE-L': np.float64(0.1998191391353057), 'BLEU': np.float64(0.030538007285949017), 'BERTScore_F1': np.float64(0.18922961143061845), 'Flesch-Kincaid Grade': np.float64(17.176181233540245)}}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
