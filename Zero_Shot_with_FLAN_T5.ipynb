{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtYqp4O70OiI"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TaCxyrQ527fV"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import textstat\n",
        "from textstat import flesch_kincaid_grade\n",
        "from scipy.stats import bootstrap\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from rouge_score import rouge_scorer\n",
        "import bert_score\n",
        "from bert_score import score, plot_example\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OHWtlYVlvaj1"
      },
      "outputs": [],
      "source": [
        "# Read files\n",
        "qa_val = pd.read_excel(\"qa_validation.xlsx\")\n",
        "qa_test = pd.read_excel(\"qa_test.xlsx\")\n",
        "\n",
        "summ_val = pd.read_excel(\"summarization_validation.xlsx\")\n",
        "summ_test = pd.read_excel(\"summarization_test.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1RJsjAn1EWG"
      },
      "source": [
        "# Zero shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "snrb1haB1DN8"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "\n",
        "\n",
        "# Build Prompt Function\n",
        "def build_prompt(row, task=\"qa\", prompt_type=\"plain\"):\n",
        "    qa_prompts = {\n",
        "        \"plain\": (\n",
        "            \"You are a medical research assistant.\\n\"\n",
        "            \"Answer the question below with only 'Yes' or 'No'.\\n\"\n",
        "            \"Do not include explanations or extra words.\\n\\n\"\n",
        "            f\"Question: {row['question']}\\n\\nAnswer (Yes or No):\"\n",
        "        ),\n",
        "        \"cite_source\": (\n",
        "            \"You are a medical research assistant.\\n\"\n",
        "            \"Answer the question below while citing possible biomedical sources, answer with only 'Yes' or 'No'.\\n\"\n",
        "            \"Do not include explanations or extra words.\\n\\n\"\n",
        "            f\"Question: {row['question']}\\n\\nAnswer (Yes or No):\"\n",
        "        ),\n",
        "        \"context\": (\n",
        "            \"You are a medical research assistant.\\n\"\n",
        "            \"Based on the context given, answer the question below with only 'Yes' or 'No'.\\n\"\n",
        "            \"Do not include explanations or extra words.\\n\\n\"\n",
        "            f\"Context: {row['context']}\\n\\nQuestion: {row['question']}\\n\\nAnswer (Yes or No):\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "    summarisation_prompts = {\n",
        "        \"plain\": (\n",
        "            \"You are a medical research assistant.\\n\"\n",
        "            \"Summarise the biomedical text below concisely:\\n\\n\"\n",
        "            f\"{row['context']}\"\n",
        "        ),\n",
        "        \"cite_source\": (\n",
        "            \"You are a medical research assistant.\\n\"\n",
        "            \"Summarise the biomedical text below concisely while citing possible biomedical sources:\\n\\n\"\n",
        "            f\"{row['context']}\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "    if task == \"qa\":\n",
        "        return qa_prompts.get(prompt_type, qa_prompts[\"plain\"])\n",
        "    elif task == \"summarisation\":\n",
        "        return summarisation_prompts.get(prompt_type, summarisation_prompts[\"plain\"])\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown task type: {task}\")\n",
        "\n",
        "\n",
        "# Generate Outputs Function\n",
        "def generate_outputs(df, task=\"qa\", prompt_type=\"plain\",\n",
        "                     max_input_len=512, max_output_len=256, num_beams=4, temperature=0.0):\n",
        "    results = []\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        prompt = build_prompt(row, task, prompt_type)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_len).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if task == \"qa\":\n",
        "                output_tokens = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=5, # short yes/no\n",
        "                    num_beams=num_beams,\n",
        "                    do_sample=True, # allow stochasticity\n",
        "                    temperature=0.7, # some randomness to avoid collapsing\n",
        "                )\n",
        "            elif task == \"summarisation\":\n",
        "                output_tokens = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_output_len,\n",
        "                    num_beams=num_beams,\n",
        "                    do_sample=False,\n",
        "                    temperature=temperature\n",
        "                )\n",
        "\n",
        "        generated = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "        results.append(generated.strip())\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def add_preds_to_df(df, all_preds):\n",
        "    df_copy = df.copy()\n",
        "    for prompt_type, preds in all_preds.items():\n",
        "        df_copy[f'pred_{prompt_type}'] = preds\n",
        "    return df_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWB8cxtqqQ24",
        "outputId": "9c3ad12b-7214-4647-de08-00d78c4f648a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2061/2061 [02:54<00:00, 11.80it/s]\n",
            "100%|██████████| 2061/2061 [02:10<00:00, 15.84it/s]\n",
            "100%|██████████| 2061/2061 [02:25<00:00, 14.17it/s]\n",
            "100%|██████████| 2061/2061 [25:05<00:00,  1.37it/s]\n",
            "100%|██████████| 2061/2061 [27:55<00:00,  1.23it/s]\n"
          ]
        }
      ],
      "source": [
        "# QA\n",
        "qa_results = {\n",
        "    prompt_type: generate_outputs(qa_test, task=\"qa\", prompt_type=prompt_type)\n",
        "    for prompt_type in [\"plain\", \"cite_source\", \"context\"]\n",
        "}\n",
        "\n",
        "qa_test_with_preds = add_preds_to_df(qa_test, qa_results)\n",
        "\n",
        "# Summarisation\n",
        "summ_results = {\n",
        "    prompt_type: generate_outputs(qa_test, task=\"summarisation\", prompt_type=prompt_type)\n",
        "    for prompt_type in [\"plain\", \"cite_source\"]\n",
        "}\n",
        "\n",
        "summ_test_with_preds = add_preds_to_df(summ_test, summ_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDl8lhRt6VCr"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cjnL0fwo6TtA"
      },
      "outputs": [],
      "source": [
        "# Decision-Based QA Metrics (Yes/No)\n",
        "def compute_qa_decision_metrics(preds, golds):\n",
        "    results = []\n",
        "    for p, g in zip(preds, golds):\n",
        "        p_clean, g_clean = str(p).lower().strip(), str(g).lower().strip()\n",
        "        em = int(p_clean == g_clean)\n",
        "        f1 = f1_score([g_clean], [p_clean], average='macro', labels=['yes','no'], zero_division=0)\n",
        "        results.append({\"Exact Match\": em, \"F1_macro\": f1})\n",
        "    return results\n",
        "\n",
        "\n",
        "# Summarisation Metrics\n",
        "def compute_text_generation_metrics(preds, golds):\n",
        "    # Include all 3 ROUGE variants\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    smoothie = SmoothingFunction().method1\n",
        "\n",
        "    # Pre-compute BERTScore\n",
        "    P, R, F1 = bert_score.score(preds, golds, lang='en', rescale_with_baseline=True)\n",
        "    bert_f1s = F1.tolist()\n",
        "\n",
        "    results = []\n",
        "    for p, g, bf1 in zip(preds, golds, bert_f1s):\n",
        "        p, g = str(p).strip(), str(g).strip()\n",
        "        if not p or not g:\n",
        "            results.append({\n",
        "                \"ROUGE-1\": 0, \"ROUGE-2\": 0, \"ROUGE-L\": 0,\n",
        "                \"BLEU\": 0, \"BERTScore_F1\": 0\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        rouge_scores = scorer.score(g, p)\n",
        "        rouge1 = rouge_scores['rouge1'].fmeasure\n",
        "        rouge2 = rouge_scores['rouge2'].fmeasure\n",
        "        rougeL = rouge_scores['rougeL'].fmeasure\n",
        "        bleu = sentence_bleu([g.split()], p.split(), smoothing_function=smoothie)\n",
        "\n",
        "        results.append({\n",
        "            \"ROUGE-1\": rouge1,\n",
        "            \"ROUGE-2\": rouge2,\n",
        "            \"ROUGE-L\": rougeL,\n",
        "            \"BLEU\": bleu,\n",
        "            \"BERTScore_F1\": bf1\n",
        "        })\n",
        "    return results\n",
        "\n",
        "\n",
        "# Readability (Flesch-Kincaid)\n",
        "def compute_readability(preds):\n",
        "    results = []\n",
        "    for p in preds:\n",
        "        p = str(p).strip()\n",
        "        grade = textstat.flesch_kincaid_grade(p) if p else None\n",
        "        results.append({\"Flesch-Kincaid Grade\": grade})\n",
        "    return results\n",
        "\n",
        "\n",
        "# Combined Evaluation Wrapper\n",
        "def evaluate_all_prompts(task, df, all_preds):\n",
        "    df_combined = df.copy()\n",
        "    all_averages = {}\n",
        "\n",
        "    if task == \"qa\":\n",
        "        for pt, preds in all_preds.items():\n",
        "            col_pred = f\"pred_{pt}\"\n",
        "            df_combined[col_pred] = preds\n",
        "\n",
        "            golds = df['final_decision'].tolist()\n",
        "            row_metrics = compute_qa_decision_metrics(preds, golds)\n",
        "\n",
        "            # Convert metrics to DataFrame and add suffix\n",
        "            metrics_df = pd.DataFrame(row_metrics).add_suffix(f\"_{pt}\")\n",
        "            df_combined = pd.concat([df_combined, metrics_df], axis=1)\n",
        "\n",
        "            # Compute dataset-level averages\n",
        "            avg_metrics = {k: np.nanmean([m[k] for m in row_metrics if m[k] is not None])\n",
        "                           for k in row_metrics[0].keys()}\n",
        "            all_averages[pt] = avg_metrics\n",
        "\n",
        "    elif task == \"summarisation\":\n",
        "        for pt, preds in all_preds.items():\n",
        "            col_pred = f\"pred_{pt}\"\n",
        "            df_combined[col_pred] = preds\n",
        "\n",
        "            golds = df['summary'].tolist()\n",
        "            gen_metrics = compute_text_generation_metrics(preds, golds)\n",
        "            read_metrics = compute_readability(preds)\n",
        "\n",
        "            row_metrics = [{**g, **r} for g, r in zip(gen_metrics, read_metrics)]\n",
        "\n",
        "            # Convert metrics to DataFrame and add suffix\n",
        "            metrics_df = pd.DataFrame(row_metrics).add_suffix(f\"_{pt}\")\n",
        "            df_combined = pd.concat([df_combined, metrics_df], axis=1)\n",
        "\n",
        "            # Compute dataset-level averages\n",
        "            avg_metrics = {k: np.nanmean([m[k] for m in row_metrics if m[k] is not None])\n",
        "                           for k in row_metrics[0].keys()}\n",
        "            all_averages[pt] = avg_metrics\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid task. Choose 'qa' or 'summarisation'\")\n",
        "\n",
        "    return df_combined, all_averages\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For QA\n",
        "qa_metrics, qa_average = evaluate_all_prompts(task=\"qa\", df=qa_test, all_preds=qa_results)\n",
        "\n",
        "# For summarisation\n",
        "summ_metrics, summ_average = evaluate_all_prompts(task=\"summarisation\", df=summ_test, all_preds=summ_results)\n",
        "\n",
        "print(\"QA:\")\n",
        "print(qa_average)\n",
        "\n",
        "print(\"\\nSummarisation:\")\n",
        "print(summ_average)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8zOi5ErmSLP",
        "outputId": "ea208107-2b70-4177-b851-0a1792e09305"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QA:\n",
            "{'plain': {'Exact Match': np.float64(0.07520621057738962), 'F1_macro': np.float64(0.03760310528869481)}, 'cite_source': {'Exact Match': np.float64(0.07520621057738962), 'F1_macro': np.float64(0.03760310528869481)}, 'context': {'Exact Match': np.float64(0.07617661329451722), 'F1_macro': np.float64(0.03808830664725861)}}\n",
            "\n",
            "Summarisation:\n",
            "{'plain': {'ROUGE-1': np.float64(0.2916620149189143), 'ROUGE-2': np.float64(0.11484886584989305), 'ROUGE-L': np.float64(0.22580086254348133), 'BLEU': np.float64(0.03569938182084287), 'BERTScore_F1': np.float64(0.21141968472655367), 'Flesch-Kincaid Grade': np.float64(16.376579970744572)}, 'cite_source': {'ROUGE-1': np.float64(0.2921941730546203), 'ROUGE-2': np.float64(0.11450630995568949), 'ROUGE-L': np.float64(0.22578845253688423), 'BLEU': np.float64(0.03588950475081277), 'BERTScore_F1': np.float64(0.20567100878938824), 'Flesch-Kincaid Grade': np.float64(16.41293031936518)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few Shot"
      ],
      "metadata": {
        "id": "_9GSWsK4sZdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_few_shot_qa_prompt(row, qa_val, n_examples=2):\n",
        "    examples = qa_val.sample(n=n_examples, random_state=int(row.name))  # random 3 examples\n",
        "    prompt = \"You are a medical research assistant.\\nAnswer the questions below with only 'Yes' or 'No'.\\nDo not include explanations or extra words.\\n\\n\"\n",
        "\n",
        "    for i, ex in enumerate(examples.itertuples(), start=1):\n",
        "        prompt += f\"Context: {ex.context}\\nQuestion: {ex.question}\\nAnswer: {ex.final_decision}\\n\\n\"\n",
        "\n",
        "    # Add target question\n",
        "    prompt += f\"Context: {row['context']}\\nQuestion: {row['question']}\\nAnswer:\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def build_few_shot_summ_prompt(row, summ_val, n_examples=2):\n",
        "    examples = summ_val.sample(n=n_examples, random_state=int(row.name))  # random 3 examples\n",
        "    prompt = \"You are a medical research assistant.\\nSummarise the biomedical texts:\\n\\n\"\n",
        "\n",
        "    for i, ex in enumerate(examples.itertuples(), start=1):\n",
        "        prompt += f\"Text: {ex.context}\\nSummary: {ex.summary}\\n\\n\"\n",
        "\n",
        "    # Add target question\n",
        "    #prompt += f\"Now, summarise the following biomedical text concisely:\\n\"\n",
        "    prompt += f\"Text: {row['context']}\\nSummary:\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def generate_outputs_fewshot(df, task=\"qa\", max_input_len=1024, max_output_len=128,\n",
        "                     num_beams=4, temperature=0.0):\n",
        "    results = []\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        # Build the prompt\n",
        "        if task == \"qa\":\n",
        "            prompt = build_few_shot_qa_prompt(row, qa_val)\n",
        "        elif task == \"summarisation\":\n",
        "            prompt = build_few_shot_summ_prompt(row, summ_val)\n",
        "        else:\n",
        "            raise ValueError(\"task must be 'qa' or 'summarisation'\")\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_len).to(model.device)\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            output_tokens = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_output_len,\n",
        "                num_beams=num_beams if task==\"summarisation\" else 1, #greedy for qa\n",
        "                do_sample=(task==\"summarisation\"),          # sampling for summarisation only\n",
        "                temperature=0.3 if task==\"summarisation\" else temperature\n",
        "            )\n",
        "\n",
        "        # Decode and store result\n",
        "        generated = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "        results.append(generated.strip())\n",
        "    return results"
      ],
      "metadata": {
        "id": "ygfINwRu0Hc2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate few-shot outputs for QA\n",
        "qa_few_shot_results = generate_outputs_fewshot(qa_test, task=\"qa\")\n",
        "\n",
        "# Generate few-shot outputs for Summarisation\n",
        "summ_few_shot_results = generate_outputs_fewshot(summ_test, task=\"summarisation\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeF2PsKDHPxR",
        "outputId": "ad9a0797-d6bb-4dfb-c303-63462f3bbae6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2061/2061 [02:15<00:00, 15.26it/s]\n",
            "100%|██████████| 2061/2061 [25:32<00:00,  1.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add few-shot predictions to the test dataframes\n",
        "qa_test_with_few_shot_preds = add_preds_to_df(qa_test, {\"few_shot\": qa_few_shot_results})\n",
        "\n",
        "summ_test_with_few_shot_preds = add_preds_to_df(summ_test, {\"few_shot\": summ_few_shot_results})"
      ],
      "metadata": {
        "id": "V9goJtmAJRCJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For QA\n",
        "qa_few_shot_metrics, qa_few_shot_average = evaluate_all_prompts(task=\"qa\", df=qa_test, all_preds={\"few_shot\": qa_few_shot_results})\n",
        "\n",
        "# For summarisation\n",
        "summ_few_shot_metrics, summ_few_shot_average = evaluate_all_prompts(task=\"summarisation\", df=summ_test, all_preds={\"few_shot\": summ_few_shot_results})\n",
        "\n",
        "print(\"QA:\")\n",
        "print(qa_few_shot_average)\n",
        "\n",
        "print(\"\\nSummarisation:\")\n",
        "print(summ_few_shot_average)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql1K1YIHMw6i",
        "outputId": "00eb55d2-51e2-40aa-fcb6-9a090e0bdbed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QA:\n",
            "{'few_shot': {'Exact Match': np.float64(0.09073265405143134), 'F1_macro': np.float64(0.04536632702571567)}}\n",
            "\n",
            "Summarisation:\n",
            "{'few_shot': {'ROUGE-1': np.float64(0.2550663889283831), 'ROUGE-2': np.float64(0.09196817559485894), 'ROUGE-L': np.float64(0.1979929710463128), 'BLEU': np.float64(0.030386339533637285), 'BERTScore_F1': np.float64(0.18712245507883937), 'Flesch-Kincaid Grade': np.float64(17.069855971767698)}}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}