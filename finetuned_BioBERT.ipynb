{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Fine-tuned BioBERT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f17878c5e993fc3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load datasets for QA\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "label_map = {\"yes\": 1, \"no\": 0}\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "train_df = pd.read_excel(\"qa_train.xlsx\")\n",
    "val_df = pd.read_excel(\"qa_validation.xlsx\")\n",
    "\n",
    "train_df[\"label\"] = train_df[\"final_decision\"].map(label_map)\n",
    "val_df[\"label\"] = val_df[\"final_decision\"].map(label_map)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"question\", \"context\", \"label\"]])\n",
    "val_dataset = Dataset.from_pandas(val_df[[\"question\", \"context\", \"label\"]])\n",
    "\n",
    "tokenised_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenised_val = val_dataset.map(preprocess_function, batched=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecc66158b5600d2a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find best hyperparameters\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "learning_rates = [5e-6, 3e-5, 5e-5]\n",
    "batch_sizes = [4, 8]\n",
    "\n",
    "best_f1 = 0\n",
    "best_args = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        print(f\"\\n Trying lr={lr}, batch_size={bs}\")\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./biobert-classification-lr{lr}-bs{bs}\",\n",
    "            do_eval=True,\n",
    "            save_strategy=\"no\",\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=bs,\n",
    "            per_device_eval_batch_size=bs,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            logging_steps=50,\n",
    "            disable_tqdm=False\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenised_train,\n",
    "            eval_dataset=tokenised_val,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        metrics = trainer.evaluate()\n",
    "\n",
    "        val_f1 = metrics.get(\"eval_f1\") or metrics.get(\"f1\", 0)\n",
    "        print(f\"Validation F1 for lr={lr}, bs={bs}: {val_f1}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_args = (lr, bs)\n",
    "\n",
    "print(\"Best hyperparameters:\", best_args)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e304c92dc50b8b4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train BioBERT using the best hyperparameters\n",
    "\n",
    "best_lr, best_bs = best_args\n",
    "\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=\"./biobert-qa-final\",\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_steps=200,\n",
    "    save_steps=400,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=best_lr,\n",
    "    per_device_train_batch_size=best_bs,\n",
    "    per_device_eval_batch_size=best_bs,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=tokenised_train,\n",
    "    eval_dataset=tokenised_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "print(metrics)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b57bc5392d0b66c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test BioBERT on test dataset\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "test_df = pd.read_excel(\"qa_test.xlsx\")\n",
    "label_map = {\"yes\": 1, \"no\": 0}\n",
    "test_df[\"label\"] = test_df[\"final_decision\"].map(label_map)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"question\", \"context\", \"label\"]])\n",
    "\n",
    "tokenised_test = test_dataset.map(preprocess_function, batched=True) \n",
    "\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=\"./biobert-qa-final\", \n",
    "    do_eval=True, \n",
    "    eval_strategy=\"no\", \n",
    "    save_strategy=\"no\", \n",
    "    learning_rate=best_lr, \n",
    "    per_device_train_batch_size=best_bs, \n",
    "    per_device_eval_batch_size=best_bs, \n",
    "    num_train_epochs=3, \n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=False, \n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "eval_trainer = Trainer(\n",
    "    model=trainer.model, # use the trained BioBERT \n",
    "    args=final_training_args,\n",
    "    tokenizer=tokenizer, \n",
    "    compute_metrics=compute_metrics, \n",
    "    eval_dataset=tokenised_test \n",
    ")\n",
    "\n",
    "metrics = eval_trainer.evaluate()\n",
    "print(metrics)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30255e8f414a4f6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print samples of QA \n",
    "\n",
    "predictions = eval_trainer.predict(tokenised_test)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Map labels back to 'Yes'/'No'\n",
    "reverse_label_map = {1: \"Yes\", 0: \"No\"}\n",
    "\n",
    "sample_indices = [498, 998, 1098]\n",
    "\n",
    "for i in sample_indices:\n",
    "    q = test_df.loc[i, \"question\"]\n",
    "    c = test_df.loc[i, \"context\"]\n",
    "    true_label = reverse_label_map[test_df.loc[i, \"label\"]]\n",
    "    pred_label = reverse_label_map[preds[i]]\n",
    "    print(f\"\\n=== Sample {i+2} ===\")\n",
    "    print(f\"Question: {q}\")\n",
    "    print(f\"Context: {c[:300]}...\") # only print first 300 characters\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(f\"Predicted Label: {pred_label}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "349aa12507a9a13e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
