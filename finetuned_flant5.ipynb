{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-28T16:47:49.127558Z",
     "start_time": "2025-10-28T16:47:38.141883Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load datasets for summarization\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "train_df_s = pd.read_excel(\"summarization_train.xlsx\")\n",
    "val_df_s = pd.read_excel(\"summarization_validation.xlsx\")\n",
    "\n",
    "# Convert to Hugging Face dataset format\n",
    "def hf_format(row):\n",
    "    return {\n",
    "        \"context\": row['context'],\n",
    "        \"summary\": row['summary']\n",
    "    }\n",
    "\n",
    "formatted = train_df_s.apply(hf_format, axis=1).tolist()\n",
    "formatted_df = pd.DataFrame(formatted)\n",
    "train_df_s = Dataset.from_pandas(formatted_df)\n",
    "\n",
    "formatted = val_df_s.apply(hf_format, axis=1).tolist()\n",
    "formatted_df = pd.DataFrame(formatted)\n",
    "val_df_s = Dataset.from_pandas(formatted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "source": [
    "Flan-T5 for summarization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "165212e97c6a9388"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "flant5 = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(flant5)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(flant5)\n",
    "\n",
    "# Preprocessing\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"context\"], max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenised_train = train_df_s.map(preprocess_function, batched=True, remove_columns=train_df_s.column_names)\n",
    "tokenised_val = val_df_s.map(preprocess_function, batched=True, remove_columns=val_df_s.column_names)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan-t5-summarization\",\n",
    "    do_eval=True,\n",
    "    eval_steps=200,\n",
    "    logging_steps=100,\n",
    "    save_steps=400,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "287a6815145421d7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define compute_metrics function and load metrics\n",
    "\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # Convert logits to token IDs if needed\n",
    "    if preds.ndim == 3:\n",
    "        preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    preds = np.asarray(preds, dtype=np.int64)\n",
    "    preds = np.clip(preds, 0, tokenizer.vocab_size - 1)\n",
    "\n",
    "    # Decode\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Clean and align\n",
    "    decoded_preds = [p.strip() for p in decoded_preds]\n",
    "    decoded_labels = [l.strip() for l in decoded_labels]\n",
    "    decoded_pairs = [(p, l) for p, l in zip(decoded_preds, decoded_labels) if p and l]\n",
    "    if not decoded_pairs:\n",
    "        return {\"rougeL\": 0.0, \"bertscore_f1\": 0.0}\n",
    "\n",
    "    decoded_preds, decoded_labels = zip(*decoded_pairs)\n",
    "\n",
    "    # Compute\n",
    "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bertscore_result = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": float(rouge_result[\"rougeL\"]),\n",
    "        \"bertscore_f1\": float(np.mean(bertscore_result[\"f1\"]))\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-28T16:48:16.753070Z",
     "start_time": "2025-10-28T16:48:12.387732Z"
    }
   },
   "id": "6c6a7061e1260645",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find best hyperparameters \n",
    "\n",
    "learning_rates = [5e-6, 3e-5, 5e-5]\n",
    "batch_sizes = [4, 8]\n",
    "\n",
    "best_rouge = 0\n",
    "best_args = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        print(f\"\\n Trying lr={lr}, batch_size={bs}\")\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=\"./flan-t5-summarization-temp\",\n",
    "            do_train=True,\n",
    "            do_eval=True,\n",
    "            eval_steps=200,\n",
    "            logging_steps=100,\n",
    "            save_steps=400,\n",
    "            save_total_limit=2,\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=bs,\n",
    "            per_device_eval_batch_size=bs,\n",
    "            num_train_epochs=3, \n",
    "            weight_decay=0.01,\n",
    "            predict_with_generate=True, \n",
    "            generation_max_length=256, # Set max summary length\n",
    "            generation_num_beams=4\n",
    "        )\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenised_train,\n",
    "            eval_dataset=tokenised_val,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        metrics = trainer.evaluate()\n",
    "        \n",
    "        val_rouge = metrics.get(\"eval_rougeL\", 0)\n",
    "        val_bert = metrics.get(\"eval_bertscore_f1\", 0)\n",
    "        \n",
    "        # Use ROUGE-L to track the best model, use BERTScore as tiebreaker \n",
    "        val_metric_for_best = val_rouge\n",
    "        print(f\"ROUGE-L F1 for lr={lr}, bs={bs}: {val_rouge}\")\n",
    "        print(f\"BERTScore F1 for lr={lr}, bs={bs}: {val_bert}\")\n",
    "\n",
    "        if val_metric_for_best > best_rouge:\n",
    "            best_rouge = val_metric_for_best\n",
    "            best_args = (lr, bs)\n",
    "\n",
    "print(\"Best hyperparameters (based on ROUGE-L):\", best_args)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "267f1b5d85f075e1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train Flan-T5 using the best hyperparameters\n",
    "\n",
    "best_lr, best_bs = best_args\n",
    "\n",
    "final_training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan-t5-summarization-final\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_steps=200,\n",
    "    logging_steps=100,\n",
    "    save_steps=400,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=best_lr,\n",
    "    per_device_train_batch_size=best_bs,\n",
    "    per_device_eval_batch_size=best_bs,\n",
    "    num_train_epochs=3, \n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True, \n",
    "    generation_max_length=256, \n",
    "    generation_num_beams=4    \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=tokenised_train,\n",
    "    eval_dataset=tokenised_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "print(metrics)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "daacf2d8014d15b5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test Flan-T5 on test dataset\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch \n",
    "from evaluate import load\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_df = pd.read_excel(\"summarization_test.xlsx\")\n",
    "\n",
    "formatted = test_df.apply(hf_format, axis=1).tolist()\n",
    "formatted_df = pd.DataFrame(formatted)\n",
    "test_df_s = Dataset.from_pandas(formatted_df)\n",
    "\n",
    "tokenised_test = test_df_s.map(preprocess_function, batched=True, remove_columns=test_df_s.column_names)\n",
    "tokenised_test.set_format(type='torch', columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "final_eval_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan-t5-summarization-test\",\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    per_device_eval_batch_size=best_bs,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=256,\n",
    "    generation_num_beams=4,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "eval_trainer = Seq2SeqTrainer(\n",
    "    model=trainer.model, # use fine-tuned Flan-t5 model\n",
    "    args=final_eval_args,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=tokenised_test\n",
    ")\n",
    "\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "test_loader = DataLoader(tokenised_test, batch_size=16)  \n",
    "\n",
    "all_preds = []\n",
    "\n",
    "trainer.model.eval()  # set to evaluation mode\n",
    "device = trainer.model.device\n",
    "\n",
    "with torch.no_grad():  # disable gradient tracking\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = trainer.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=128,\n",
    "            num_beams=4\n",
    "        )\n",
    "        decoded_batch = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        all_preds.extend(decoded_batch)\n",
    "\n",
    "decoded_preds = all_preds\n",
    "\n",
    "decoded_labels = tokenizer.batch_decode(\n",
    "    tokenised_test[\"labels\"],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Compute ROUGE-1/2/L scores  \n",
    "rouge_scores = rouge.compute(\n",
    "    predictions=decoded_preds,\n",
    "    references=decoded_labels,\n",
    "    rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    ")\n",
    "\n",
    "metrics = eval_trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "print(\"\\nROUGE-1:\", rouge_scores[\"rouge1\"])\n",
    "print(\"ROUGE-2:\", rouge_scores[\"rouge2\"])\n",
    "print(\"ROUGE-L:\", rouge_scores[\"rougeL\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "387c08a5de50a30c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute readability metrics (Flesch-Kincaid)\n",
    "\n",
    "from textstat import flesch_kincaid_grade, flesch_reading_ease\n",
    "\n",
    "print(\"\\nSample generated summaries with readability metrics:\")\n",
    "\n",
    "sample_indices = (9, 99, 499, 999, 1999)\n",
    "\n",
    "for i in sample_indices:\n",
    "    pred = decoded_preds[i].strip()\n",
    "    label = decoded_labels[i].strip()\n",
    "    fk_pred = flesch_kincaid_grade(pred)\n",
    "    fk_label = flesch_kincaid_grade(label)\n",
    "    fre_pred = flesch_reading_ease(pred)\n",
    "    fre_label = flesch_reading_ease(label)\n",
    "\n",
    "    print(f\"\\n Sample {i+1}\")\n",
    "    print(f\"Predicted Summary:\\n{pred}\")\n",
    "    print(f\"Actual Summary:\\n{label}\")\n",
    "    print(f\"Flesch–Kincaid Grade (Pred): {fk_pred:.2f}, (Label): {fk_label:.2f}\")\n",
    "    print(f\"Flesch Reading Ease (Pred): {fre_pred:.2f}, (Label): {fre_label:.2f}\")\n",
    "\n",
    "# Compute average readability for the entire test set\n",
    "fk_pred_all = []\n",
    "fk_label_all = []\n",
    "fre_pred_all = []\n",
    "fre_label_all = []\n",
    "\n",
    "for pred, label in zip(decoded_preds, decoded_labels):\n",
    "    pred = pred.strip()\n",
    "    label = label.strip()\n",
    "    if pred and label:  # skip empty\n",
    "        fk_pred_all.append(flesch_kincaid_grade(pred))\n",
    "        fk_label_all.append(flesch_kincaid_grade(label))\n",
    "        fre_pred_all.append(flesch_reading_ease(pred))\n",
    "        fre_label_all.append(flesch_reading_ease(label))\n",
    "\n",
    "avg_fk_pred = sum(fk_pred_all) / len(fk_pred_all)\n",
    "avg_fk_label = sum(fk_label_all) / len(fk_label_all)\n",
    "avg_fre_pred = sum(fre_pred_all) / len(fre_pred_all)\n",
    "avg_fre_label = sum(fre_label_all) / len(fre_label_all)\n",
    "\n",
    "print(\"\\nAverage readability scores for entire test set:\")\n",
    "print(f\"Flesch–Kincaid Grade (Pred): {avg_fk_pred:.2f}, (Label): {avg_fk_label:.2f}\")\n",
    "print(f\"Flesch Reading Ease (Pred): {avg_fre_pred:.2f}, (Label): {avg_fre_label:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad074636cac8b98c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
