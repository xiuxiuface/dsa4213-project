{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAG Summarization - Complete Pipeline\n\n##  Project Workflow\n1. ✅ Set up retrieval (BM25 & FAISS dense)\n2. ✅ Implement RAG pipeline\n3. ✅ Ablation studies (retriever type, top-k)\n4. ✅ Evaluate QA & summarisation, compute readability metrics\n5. ✅ Log and compare ablation results\n\n##  Evaluation Metrics\n- ROUGE-1, ROUGE-2, ROUGE-L\n- F1 BERTScore\n- Avg Flesch-Kincaid Grade\n- Individual FK grades (in CSV)","metadata":{}},{"cell_type":"code","source":" # Define your branch name as a string\nBRANCH = \"yunxiu-branch\"\n\n# Clone your GitHub repo and switch to your branch\n!git clone -b $BRANCH https://github.com/xiuxiuface/dsa4213-project.git\n\n# Move into the project folder\n%cd dsa4213-project","metadata":{"execution":{"iopub.status.busy":"2025-11-13T17:16:20.889260Z","iopub.execute_input":"2025-11-13T17:16:20.889516Z","iopub.status.idle":"2025-11-13T17:16:27.025690Z","shell.execute_reply.started":"2025-11-13T17:16:20.889491Z","shell.execute_reply":"2025-11-13T17:16:27.024716Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'dsa4213-project'...\nremote: Enumerating objects: 248, done.\u001b[K\nremote: Counting objects: 100% (160/160), done.\u001b[K\nremote: Compressing objects: 100% (132/132), done.\u001b[K\nremote: Total 248 (delta 85), reused 56 (delta 27), pack-reused 88 (from 2)\u001b[K\nReceiving objects: 100% (248/248), 84.72 MiB | 29.64 MiB/s, done.\nResolving deltas: 100% (98/98), done.\n/kaggle/working/dsa4213-project\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Installation (Run Once)","metadata":{}},{"cell_type":"code","source":"!pip install -q rank-bm25 sentence-transformers faiss-cpu transformers torch rouge-score bert-score textstat pandas numpy tqdm openpyxl","metadata":{"execution":{"iopub.status.busy":"2025-11-13T17:16:27.027465Z","iopub.execute_input":"2025-11-13T17:16:27.027698Z","iopub.status.idle":"2025-11-13T17:17:40.382931Z","shell.execute_reply.started":"2025-11-13T17:16:27.027675Z","shell.execute_reply":"2025-11-13T17:17:40.382234Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport os\nfrom typing import List, Dict, Tuple\n\n# Retrieval\nfrom rank_bm25 import BM25Okapi\nfrom sentence_transformers import SentenceTransformer\nimport faiss\n\n# Generation\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# Evaluation\nfrom rouge_score import rouge_scorer\nfrom bert_score import score as bert_score\nimport textstat\n","metadata":{"execution":{"iopub.status.busy":"2025-11-13T17:17:40.384027Z","iopub.execute_input":"2025-11-13T17:17:40.384344Z","iopub.status.idle":"2025-11-13T17:18:13.972058Z","shell.execute_reply.started":"2025-11-13T17:17:40.384306Z","shell.execute_reply":"2025-11-13T17:18:13.971473Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-11-13 17:17:53.872077: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763054274.057973      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763054274.109476      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":3},{"cell_type":"markdown","source":"## 1️⃣ Data Loading","metadata":{}},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_excel(\"rag_dataset.xlsx\", engine='openpyxl')\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Columns: {df.columns.tolist()}\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2025-11-13T17:18:13.972892Z","iopub.execute_input":"2025-11-13T17:18:13.973600Z","iopub.status.idle":"2025-11-13T17:18:18.100379Z","shell.execute_reply.started":"2025-11-13T17:18:13.973578Z","shell.execute_reply":"2025-11-13T17:18:18.099686Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Dataset shape: (46610, 5)\nColumns: ['question_id', 'question', 'answer', 'passage', 'passage_id']\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   question_id                                           question  \\\n0            0  Is hidradenitis suppurativa a systemic disease...   \n1            0  Is hidradenitis suppurativa a systemic disease...   \n2            0  Is hidradenitis suppurativa a systemic disease...   \n3            1  Is admission hyperglycemia associated with fai...   \n4            1  Is admission hyperglycemia associated with fai...   \n\n                                              answer  \\\n0  Control subjects were not validated for absenc...   \n1  Control subjects were not validated for absenc...   \n2  Control subjects were not validated for absenc...   \n3  In patients with STEMI who undergo FT, admissi...   \n4  In patients with STEMI who undergo FT, admissi...   \n\n                                             passage  passage_id  \n0  Hidradenitis suppurativa (HS) is a chronic inf...           0  \n1  In this retrospective case-control study, we c...           1  \n2  A total of 2292 patients at Massachusetts Gene...           2  \n3  Hyperglycemia on admission is associated with ...           3  \n4  This is a retrospective study of 304 STEMI pat...           4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question_id</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>passage</th>\n      <th>passage_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Is hidradenitis suppurativa a systemic disease...</td>\n      <td>Control subjects were not validated for absenc...</td>\n      <td>Hidradenitis suppurativa (HS) is a chronic inf...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Is hidradenitis suppurativa a systemic disease...</td>\n      <td>Control subjects were not validated for absenc...</td>\n      <td>In this retrospective case-control study, we c...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>Is hidradenitis suppurativa a systemic disease...</td>\n      <td>Control subjects were not validated for absenc...</td>\n      <td>A total of 2292 patients at Massachusetts Gene...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Is admission hyperglycemia associated with fai...</td>\n      <td>In patients with STEMI who undergo FT, admissi...</td>\n      <td>Hyperglycemia on admission is associated with ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>Is admission hyperglycemia associated with fai...</td>\n      <td>In patients with STEMI who undergo FT, admissi...</td>\n      <td>This is a retrospective study of 304 STEMI pat...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Prepare corpus (unique passages)\nunique_passages = df.drop_duplicates(subset=[\"passage_id\"])\nunique_passages = unique_passages[unique_passages['passage'].notna()]\ncorpus = unique_passages[\"passage\"].tolist()\npassage_ids = unique_passages[\"passage_id\"].tolist()\n\nprint(f\"Total unique passages: {len(corpus)}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-13T17:18:18.102151Z","iopub.execute_input":"2025-11-13T17:18:18.111984Z","iopub.status.idle":"2025-11-13T17:18:18.133045Z","shell.execute_reply.started":"2025-11-13T17:18:18.111961Z","shell.execute_reply":"2025-11-13T17:18:18.132338Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Total unique passages: 46609\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 2️⃣ Retriever Classes","metadata":{}},{"cell_type":"code","source":"# BM25 Retriever\nclass BM25Retriever:\n    def __init__(self, corpus: List[str]):\n        print(\"Initializing BM25...\")\n        tokenized_corpus = [doc.lower().split() for doc in corpus]\n        self.bm25 = BM25Okapi(tokenized_corpus)\n        self.corpus = corpus\n        print(\"✓ BM25 ready\")\n    \n    def retrieve(self, query: str, k: int = 3) -> List[str]:\n        tokenized_query = query.lower().split()\n        scores = self.bm25.get_scores(tokenized_query)\n        top_k_indices = np.argsort(scores)[::-1][:k]\n        return [self.corpus[i] for i in top_k_indices]","metadata":{"execution":{"iopub.status.busy":"2025-11-13T17:18:18.133780Z","iopub.execute_input":"2025-11-13T17:18:18.134045Z","iopub.status.idle":"2025-11-13T17:18:18.139074Z","shell.execute_reply.started":"2025-11-13T17:18:18.134022Z","shell.execute_reply":"2025-11-13T17:18:18.138383Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# FAISS Dense Retriever\nclass FAISSRetriever:\n    def __init__(self, corpus: List[str], model_name: str = 'all-MiniLM-L6-v2'):\n        print(f\"Initializing FAISS with {model_name}...\")\n        self.embed_model = SentenceTransformer(model_name)\n        self.corpus = corpus\n        \n        # Create embeddings\n        print(\"Creating passage embeddings...\")\n        self.passage_embeddings = self.embed_model.encode(\n            corpus, \n            show_progress_bar=True,\n            convert_to_numpy=True\n        )\n        \n        # Build FAISS index\n        dimension = self.passage_embeddings.shape[1]\n        self.index = faiss.IndexFlatIP(dimension)  # Inner Product\n        \n        # Normalize for cosine similarity\n        faiss.normalize_L2(self.passage_embeddings)\n        self.index.add(self.passage_embeddings)\n        print(f\"✓ FAISS index ready with {self.index.ntotal} vectors\")\n    \n    def retrieve(self, query: str, k: int = 3) -> List[str]:\n        query_vec = self.embed_model.encode([query], convert_to_numpy=True)\n        faiss.normalize_L2(query_vec)\n        scores, indices = self.index.search(query_vec, k)\n        return [self.corpus[i] for i in indices[0]]","metadata":{"execution":{"iopub.status.busy":"2025-11-13T17:18:18.139837Z","iopub.execute_input":"2025-11-13T17:18:18.140068Z","iopub.status.idle":"2025-11-13T17:18:18.151431Z","shell.execute_reply.started":"2025-11-13T17:18:18.140033Z","shell.execute_reply":"2025-11-13T17:18:18.150815Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## 3️⃣ Initialize Retrievers","metadata":{}},{"cell_type":"code","source":"# Initialize BM25\nbm25_retriever = BM25Retriever(corpus)","metadata":{"execution":{"iopub.status.busy":"2025-11-13T17:18:18.152024Z","iopub.execute_input":"2025-11-13T17:18:18.152244Z","iopub.status.idle":"2025-11-13T17:18:19.992816Z","shell.execute_reply.started":"2025-11-13T17:18:18.152221Z","shell.execute_reply":"2025-11-13T17:18:19.992176Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Initializing BM25...\n✓ BM25 ready\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Initialize FAISS\nfaiss_retriever = FAISSRetriever(corpus)","metadata":{"execution":{"iopub.status.busy":"2025-11-13T17:18:19.993747Z","iopub.execute_input":"2025-11-13T17:18:19.994023Z","iopub.status.idle":"2025-11-13T17:19:30.802583Z","shell.execute_reply.started":"2025-11-13T17:18:19.993995Z","shell.execute_reply":"2025-11-13T17:19:30.801918Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Initializing FAISS with all-MiniLM-L6-v2...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59a744384dc64b25aa67437b1bdc3ba1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d948da68aa5d4958ae5131c515c5cad8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59cccffe59384ab796e0289748e5c709"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df64e3c2834941999fe6e61748ea9ab2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85d2ccbd060a47849911659f858ca9e7"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10276205a996423abd11050a6ee2f4ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e78d6336995d4733b4b7fb0b8d964cd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6ee43af8ceb41f293a936a9ba62bd0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02109cc858fb4d91a8f6760251b7f02f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58cc980aa18c4b8dbe0ec09b4d315dfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b13df65768174865b168c4c061feb642"}},"metadata":{}},{"name":"stdout","text":"Creating passage embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1457 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca6143aa2ef34c08a60fc6240cd35a01"}},"metadata":{}},{"name":"stdout","text":"✓ FAISS index ready with 46609 vectors\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 4️⃣ Summarization Model","metadata":{}},{"cell_type":"code","source":"summarisation_prompts = {\n    \"plain\": \"You are a medical research assistant.\\nSummarise the biomedical text below concisely:\\n\\n\",\n    \"cite_source\": \"You are a medical research assistant.\\nSummarise the biomedical text below concisely while citing possible biomedical sources:\\n\\n\"\n}\nclass SummarizationModel:\n    def __init__(self, model_name: str = \"google/flan-t5-small\"):\n        print(f\"Loading {model_name}...\")\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n        print(f\"✓ Model loaded on {self.device}\")\n    \n    def generate(self, question: str, context: List[str], max_length: int = 150, prompt_type: str = \"plain\") -> str:\n        context_str = \" \".join(context)\n        prompt = summarisation_prompts[prompt_type] + context_str\n        \n        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_length=max_length,\n                num_beams=4,\n                early_stopping=True\n            )\n        \n        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Initialize\nsummarizer = SummarizationModel()","metadata":{"execution":{"iopub.status.busy":"2025-11-13T18:41:51.603699Z","iopub.execute_input":"2025-11-13T18:41:51.604013Z","iopub.status.idle":"2025-11-13T18:41:52.451905Z","shell.execute_reply.started":"2025-11-13T18:41:51.603991Z","shell.execute_reply":"2025-11-13T18:41:52.451257Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loading google/flan-t5-small...\n✓ Model loaded on cuda\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## 5️⃣ Quick Test","metadata":{}},{"cell_type":"code","source":"# Test with one example\ntest_row = df.drop_duplicates(subset=[\"question_id\"]).iloc[0]\n\nprint(\"Question:\", test_row[\"question\"])\nprint(\"\\nTrue answer:\", test_row[\"answer\"])\n\n# BM25\nprint(\"\\n\" + \"=\"*60)\nprint(\"BM25 Retrieval (k=3):\")\nbm25_ctx = bm25_retriever.retrieve(test_row[\"question\"], k=3)\nbm25_summary = summarizer.generate(test_row[\"question\"], bm25_ctx)\nprint(\"Summary:\", bm25_summary)\nprint(\"FK Grade:\", textstat.flesch_kincaid_grade(bm25_summary))\n\n# FAISS\nprint(\"\\n\" + \"=\"*60)\nprint(\"FAISS Retrieval (k=3):\")\nfaiss_ctx = faiss_retriever.retrieve(test_row[\"question\"], k=3)\nfaiss_summary = summarizer.generate(test_row[\"question\"], faiss_ctx)\nprint(\"Summary:\", faiss_summary)\nprint(\"FK Grade:\", textstat.flesch_kincaid_grade(faiss_summary))","metadata":{"execution":{"iopub.status.busy":"2025-11-13T18:41:56.979631Z","iopub.execute_input":"2025-11-13T18:41:56.979942Z","iopub.status.idle":"2025-11-13T18:41:57.949843Z","shell.execute_reply.started":"2025-11-13T18:41:56.979920Z","shell.execute_reply":"2025-11-13T18:41:57.949245Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Question: Is hidradenitis suppurativa a systemic disease with substantial comorbidity burden?\n\nTrue answer: Control subjects were not validated for absence of HS and comorbidity validation was not performed for either group.\n\n============================================================\nBM25 Retrieval (k=3):\nSummary: The prevalence and comorbidities of Hidradenitis suppurativa in diabetic patients.\nFK Grade: 16.63\n\n============================================================\nFAISS Retrieval (k=3):\nSummary: Prevalence and comorbidity of HS in a large patient care database.\nFK Grade: 10.154545454545456\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## 6️⃣ Evaluation Functions","metadata":{}},{"cell_type":"code","source":"class RAGEvaluator:\n    def __init__(self):\n        self.rouge_scorer = rouge_scorer.RougeScorer(\n            ['rouge1', 'rouge2', 'rougeL'], \n            use_stemmer=True\n        )\n    \n    def evaluate_batch(self, predictions: List[str], references: List[str]) -> Tuple[Dict, List]:\n        # ROUGE scores\n        rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n        \n        for pred, ref in zip(predictions, references):\n            scores = self.rouge_scorer.score(ref, pred)\n            rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n            rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n            rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n        \n        # BERTScore\n        P, R, F1 = bert_score(predictions, references, lang=\"en\", verbose=False)\n        \n        # Flesch-Kincaid\n        fk_grades = [textstat.flesch_kincaid_grade(p) for p in predictions]\n        \n        metrics = {\n            \"ROUGE-1\": np.mean(rouge_scores['rouge1']),\n            \"ROUGE-2\": np.mean(rouge_scores['rouge2']),\n            \"ROUGE-L\": np.mean(rouge_scores['rougeL']),\n            \"BERTScore_F1\": F1.mean().item(),\n            \"Avg_FK_Grade\": np.mean(fk_grades)\n        }\n        \n        return metrics, fk_grades\n\nevaluator = RAGEvaluator()\nprint(\"✓ Evaluator ready\")","metadata":{"execution":{"iopub.status.busy":"2025-11-13T18:42:01.196948Z","iopub.execute_input":"2025-11-13T18:42:01.197273Z","iopub.status.idle":"2025-11-13T18:42:01.204758Z","shell.execute_reply.started":"2025-11-13T18:42:01.197251Z","shell.execute_reply":"2025-11-13T18:42:01.203896Z"},"trusted":true},"outputs":[{"name":"stdout","text":"✓ Evaluator ready\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## 7️⃣ Ablation Study (We decided to use only FAISS cuz it outperformed BM25)","metadata":{}},{"cell_type":"code","source":"# Sample test set for ablation\nTEST_SIZE = 500  # Adjust based on your computational resources\ntest_df = df.drop_duplicates(subset=[\"question_id\"]).sample(n=TEST_SIZE, random_state=42)\n\nprint(f\"Test set size: {len(test_df)}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-13T18:42:04.335739Z","iopub.execute_input":"2025-11-13T18:42:04.336610Z","iopub.status.idle":"2025-11-13T18:42:04.346132Z","shell.execute_reply.started":"2025-11-13T18:42:04.336584Z","shell.execute_reply":"2025-11-13T18:42:04.345359Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Test set size: 500\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Ablation configurations\nconfigs = [\n    {\"name\": \"FAISS_k1_plain\", \"retriever\": faiss_retriever, \"k\": 1, \"prompt_type\": \"plain\"},\n    {\"name\": \"FAISS_k1_cite\", \"retriever\": faiss_retriever, \"k\": 1, \"prompt_type\": \"cite_source\"},\n    {\"name\": \"FAISS_k3_plain\", \"retriever\": faiss_retriever, \"k\": 3, \"prompt_type\": \"plain\"},\n    {\"name\": \"FAISS_k3_cite\", \"retriever\": faiss_retriever, \"k\": 3, \"prompt_type\": \"cite_source\"},\n    {\"name\": \"FAISS_k5_plain\", \"retriever\": faiss_retriever, \"k\": 5, \"prompt_type\": \"plain\"},\n    {\"name\": \"FAISS_k5_cite\", \"retriever\": faiss_retriever, \"k\": 5, \"prompt_type\": \"cite_source\"},\n]\n\nablation_results = []\n\nfor config in configs:\n    print(f\"\\n{'='*60}\")\n    print(f\"Running: {config['name']}\")\n    print(f\"{'='*60}\")\n    \n    predictions = []\n    references = []\n    \n    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=config['name']):\n        ctx = config['retriever'].retrieve(row['question'], k=config['k'])\n        pred = summarizer.generate(row['question'], ctx, prompt_type=config['prompt_type'])\n        predictions.append(pred)\n        references.append(row['answer'])\n    \n    # Evaluate\n    metrics, _ = evaluator.evaluate_batch(predictions, references)\n    \n    result = {\n        \"config_name\": config['name'],\n        \"retriever_type\": config['name'].split('_')[0],\n        \"k\": config['k'],\n        \"prompt_type\": config.get('prompt_type', 'plain'),  # Add this line\n        **metrics\n    }\n    ablation_results.append(result)\n    \n    print(f\"\\nResults:\")\n    for metric, value in metrics.items():\n        print(f\"  {metric}: {value:.4f}\")\n\n# Save results\nos.makedirs(\"results\", exist_ok=True)\n# Group results by prompt type and save separately\nablation_df = pd.DataFrame(ablation_results)\nablation_df.to_csv(\"results/ablation_study_all.csv\", index=False)\n\n# Save separate files for each prompt type\n# Select best config for EACH prompt type and run full evaluation\nfor prompt_type in ['plain', 'cite_source']:\n    prompt_results = ablation_df[ablation_df['prompt_type'] == prompt_type]\n    best_config = prompt_results.loc[prompt_results['BERTScore_F1'].idxmax()]\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"Best configuration for {prompt_type}:\")\n    print(best_config)\n    print(f\"{'='*80}\")\n    \n    # Use the best retriever and k for this prompt type\n    BEST_RETRIEVER = faiss_retriever if \"FAISS\" in best_config['config_name'] else bm25_retriever\n    BEST_K = int(best_config['k'])\n    \n    # Full evaluation on entire dataset\n    print(f\"\\nRunning full evaluation for {prompt_type}...\")\n    print(f\"Using: {best_config['config_name']}\")\n    \n    unique_df = df.drop_duplicates(subset=[\"question_id\"]).reset_index(drop=True)\n    \n    predictions = []\n    references = []\n    \n    for _, row in tqdm(unique_df.iterrows(), total=len(unique_df), desc=f\"Full eval - {prompt_type}\"):\n        ctx = BEST_RETRIEVER.retrieve(row['question'], k=BEST_K)\n        pred = summarizer.generate(row['question'], ctx, prompt_type=prompt_type)\n        predictions.append(pred)\n        references.append(row['answer'])\n    \n    # Add predictions to dataframe\n    unique_df[f'rag_summary_{prompt_type}'] = predictions\n    \n    # Evaluate\n    final_metrics, fk_grades = evaluator.evaluate_batch(predictions, references)\n    unique_df[f'FK_grade_{prompt_type}'] = fk_grades\n    \n    # Save\n    unique_df.to_csv(f\"results/rag_full_outputs_{prompt_type}.csv\", index=False)\n    \n    with open(f\"results/rag_full_metrics_{prompt_type}.json\", \"w\") as f:\n        json.dump(final_metrics, f, indent=2)\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"FINAL METRICS for {prompt_type}\")\n    print(f\"{'='*80}\")\n    for metric, value in final_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    print(f\"\\n✓ Saved to results/\")","metadata":{"execution":{"iopub.status.busy":"2025-11-13T18:44:24.940185Z","iopub.execute_input":"2025-11-13T18:44:24.940907Z","iopub.status.idle":"2025-11-13T23:28:05.787946Z","shell.execute_reply.started":"2025-11-13T18:44:24.940883Z","shell.execute_reply":"2025-11-13T23:28:05.787140Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n============================================================\nRunning: FAISS_k1_plain\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"FAISS_k1_plain: 100%|██████████| 500/500 [04:33<00:00,  1.83it/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nResults:\n  ROUGE-1: 0.2714\n  ROUGE-2: 0.0980\n  ROUGE-L: 0.2112\n  BERTScore_F1: 0.8661\n  Avg_FK_Grade: 15.5416\n\n============================================================\nRunning: FAISS_k1_cite\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"FAISS_k1_cite: 100%|██████████| 500/500 [04:36<00:00,  1.81it/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nResults:\n  ROUGE-1: 0.2748\n  ROUGE-2: 0.0997\n  ROUGE-L: 0.2112\n  BERTScore_F1: 0.8666\n  Avg_FK_Grade: 15.7351\n\n============================================================\nRunning: FAISS_k3_plain\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"FAISS_k3_plain: 100%|██████████| 500/500 [04:24<00:00,  1.89it/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nResults:\n  ROUGE-1: 0.2473\n  ROUGE-2: 0.0850\n  ROUGE-L: 0.1953\n  BERTScore_F1: 0.8637\n  Avg_FK_Grade: 15.6739\n\n============================================================\nRunning: FAISS_k3_cite\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"FAISS_k3_cite: 100%|██████████| 500/500 [04:28<00:00,  1.86it/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nResults:\n  ROUGE-1: 0.2476\n  ROUGE-2: 0.0836\n  ROUGE-L: 0.1951\n  BERTScore_F1: 0.8637\n  Avg_FK_Grade: 15.9728\n\n============================================================\nRunning: FAISS_k5_plain\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"FAISS_k5_plain: 100%|██████████| 500/500 [04:13<00:00,  1.97it/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nResults:\n  ROUGE-1: 0.2305\n  ROUGE-2: 0.0773\n  ROUGE-L: 0.1817\n  BERTScore_F1: 0.8592\n  Avg_FK_Grade: 15.4355\n\n============================================================\nRunning: FAISS_k5_cite\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"FAISS_k5_cite: 100%|██████████| 500/500 [04:09<00:00,  2.00it/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nResults:\n  ROUGE-1: 0.2211\n  ROUGE-2: 0.0746\n  ROUGE-L: 0.1728\n  BERTScore_F1: 0.8573\n  Avg_FK_Grade: 15.4645\n\n================================================================================\nBest configuration for plain:\nconfig_name       FAISS_k1_plain\nretriever_type             FAISS\nk                              1\nprompt_type                plain\nROUGE-1                 0.271444\nROUGE-2                  0.09801\nROUGE-L                 0.211209\nBERTScore_F1            0.866089\nAvg_FK_Grade           15.541614\nName: 0, dtype: object\n================================================================================\n\nRunning full evaluation for plain...\nUsing: FAISS_k1_plain\n","output_type":"stream"},{"name":"stderr","text":"Full eval - plain: 100%|██████████| 13738/13738 [2:03:07<00:00,  1.86it/s]  \nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nFINAL METRICS for plain\n================================================================================\nROUGE-1: 0.2686\nROUGE-2: 0.0962\nROUGE-L: 0.2081\nBERTScore_F1: 0.8657\nAvg_FK_Grade: 15.5026\n\n✓ Saved to results/\n\n================================================================================\nBest configuration for cite_source:\nconfig_name       FAISS_k1_cite\nretriever_type            FAISS\nk                             1\nprompt_type         cite_source\nROUGE-1                0.274784\nROUGE-2                0.099689\nROUGE-L                0.211238\nBERTScore_F1           0.866593\nAvg_FK_Grade          15.735061\nName: 1, dtype: object\n================================================================================\n\nRunning full evaluation for cite_source...\nUsing: FAISS_k1_cite\n","output_type":"stream"},{"name":"stderr","text":"Full eval - cite_source: 100%|██████████| 13738/13738 [2:04:02<00:00,  1.85it/s]  \nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nFINAL METRICS for cite_source\n================================================================================\nROUGE-1: 0.2668\nROUGE-2: 0.0959\nROUGE-L: 0.2073\nBERTScore_F1: 0.8651\nAvg_FK_Grade: 15.4753\n\n✓ Saved to results/\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## 8️⃣ Full Evaluation\n","metadata":{}},{"cell_type":"code","source":"# Select best config from ablation\nbest_config = ablation_df.loc[ablation_df['BERTScore_F1'].idxmax()]\nprint(\"Best configuration:\")\nprint(best_config)\n\n# Use the best retriever and k\nBEST_RETRIEVER = faiss_retriever if \"FAISS\" in best_config['config_name'] else bm25_retriever\nBEST_K = int(best_config['k'])","metadata":{"execution":{"iopub.status.busy":"2025-11-13T23:28:05.789172Z","iopub.execute_input":"2025-11-13T23:28:05.789413Z","iopub.status.idle":"2025-11-13T23:28:05.795124Z","shell.execute_reply.started":"2025-11-13T23:28:05.789391Z","shell.execute_reply":"2025-11-13T23:28:05.794429Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Best configuration:\nconfig_name       FAISS_k1_cite\nretriever_type            FAISS\nk                             1\nprompt_type         cite_source\nROUGE-1                0.274784\nROUGE-2                0.099689\nROUGE-L                0.211238\nBERTScore_F1           0.866593\nAvg_FK_Grade          15.735061\nName: 1, dtype: object\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Full evaluation on entire dataset\nprint(\"Running full evaluation...\")\nprint(f\"Using: {best_config['config_name']}\")\n\nunique_df = df.drop_duplicates(subset=[\"question_id\"]).reset_index(drop=True)\n\npredictions = []\nreferences = []\n\nfor _, row in tqdm(unique_df.iterrows(), total=len(unique_df), desc=\"Full evaluation\"):\n    ctx = BEST_RETRIEVER.retrieve(row['question'], k=BEST_K)\n    pred = summarizer.generate(row['question'], ctx)\n    predictions.append(pred)\n    references.append(row['answer'])\n\n# Add predictions to dataframe\nunique_df['rag_summary'] = predictions\n\n# Evaluate\nfinal_metrics, fk_grades = evaluator.evaluate_batch(predictions, references)\nunique_df['FK_grade'] = fk_grades\n\n# Save\nunique_df.to_csv(\"results/rag_full_outputs.csv\", index=False)\n\nwith open(\"results/rag_full_metrics.json\", \"w\") as f:\n    json.dump(final_metrics, f, indent=2)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL METRICS\")\nprint(\"=\"*80)\nfor metric, value in final_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n\nprint(f\"\\n✓ Saved to results/\")","metadata":{"execution":{"iopub.status.busy":"2025-11-13T23:37:44.060137Z","iopub.execute_input":"2025-11-13T23:37:44.060757Z","iopub.status.idle":"2025-11-14T01:45:26.587965Z","shell.execute_reply.started":"2025-11-13T23:37:44.060734Z","shell.execute_reply":"2025-11-14T01:45:26.587315Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Running full evaluation...\nUsing: FAISS_k1_cite\n","output_type":"stream"},{"name":"stderr","text":"Full evaluation: 100%|██████████| 13738/13738 [2:03:18<00:00,  1.86it/s]  \nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nFINAL METRICS\n================================================================================\nROUGE-1: 0.2686\nROUGE-2: 0.0962\nROUGE-L: 0.2081\nBERTScore_F1: 0.8657\nAvg_FK_Grade: 15.5026\n\n✓ Saved to results/\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## 9️⃣ Extract Required Passages","metadata":{}},{"cell_type":"code","source":"# Extract specific passages\nrequired_passages = [16771, 12220, 29568]\n\nsubset_df = unique_df[unique_df['passage_id'].isin(required_passages)].copy()\nsubset_df.to_csv(\"results/rag_required_passages.csv\", index=False)\n\nprint(\"Required passages extracted:\")\nsubset_df[['passage_id', 'question', 'rag_summary', 'FK_grade']]","metadata":{"execution":{"iopub.status.busy":"2025-11-14T01:45:26.589270Z","iopub.execute_input":"2025-11-14T01:45:26.589509Z","iopub.status.idle":"2025-11-14T01:45:26.601331Z","shell.execute_reply.started":"2025-11-14T01:45:26.589492Z","shell.execute_reply":"2025-11-14T01:45:26.600708Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Required passages extracted:\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"      passage_id                                           question  \\\n3601       12220  Is parathyroid hormone associated with biomark...   \n4953       16771  Does iGF-2 mediate intestinal mucosal hyperpla...   \n8722       29568  Is insulin-like growth factor binding protein-...   \n\n                                            rag_summary   FK_grade  \n3601  The association of 25-hydroxyvitamin D and tes...  23.761667  \n4953  IGF2 and IGF1R mediate the Rb-IKO intestinal p...   9.655000  \n8722  Plasma IGFBP2 levels are associated with clini...  12.690000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>passage_id</th>\n      <th>question</th>\n      <th>rag_summary</th>\n      <th>FK_grade</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3601</th>\n      <td>12220</td>\n      <td>Is parathyroid hormone associated with biomark...</td>\n      <td>The association of 25-hydroxyvitamin D and tes...</td>\n      <td>23.761667</td>\n    </tr>\n    <tr>\n      <th>4953</th>\n      <td>16771</td>\n      <td>Does iGF-2 mediate intestinal mucosal hyperpla...</td>\n      <td>IGF2 and IGF1R mediate the Rb-IKO intestinal p...</td>\n      <td>9.655000</td>\n    </tr>\n    <tr>\n      <th>8722</th>\n      <td>29568</td>\n      <td>Is insulin-like growth factor binding protein-...</td>\n      <td>Plasma IGFBP2 levels are associated with clini...</td>\n      <td>12.690000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"## Summary\n\n### Generated Files:\n1. `results/ablation_study_all.csv` - \n2. `results/rag_full_outputs_cite_source.csv` -\n3. `results/rag_full_outputs_plain.csv` -\n4. `results/rag_full_metrics_plain.json` -\n5. `results/rag_full_metrics_cite_source.json` - \n6. `results/rag_required_passages.csv` - Specific passages [16771, 12220, 29568]\n\n### Metrics in JSON:\n- ROUGE-1\n- ROUGE-2\n- ROUGE-L\n- BERTScore_F1\n- Avg_FK_Grade ← **Average across all samples**\n\n### Individual FK Grades:\n- Stored in CSV files (column: `FK_grade`)","metadata":{}}]}